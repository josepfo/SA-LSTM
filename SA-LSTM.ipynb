{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math, time\n",
    "import datetime\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "# fixar random seed para se puder reproduzir os resultados\n",
    "seed = 9\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_data(stock_name, normalized=0,file_name=None):\n",
    "    if not file_name:\n",
    "        file_name ='table.csv'%stock_name\n",
    "    col_names = ['Date','Open','High','Low','Close','Volume','Adj Close']\n",
    "    stocks = pd.read_csv(file_name, header=0, names=col_names) #fica numa especie de tabela exactamente como estava no csv (1350 linhas,7 colunas)\n",
    "    df = pd.DataFrame(stocks) #neste caso não vai fazer nada\n",
    "    date_split = df['Date'].str.split('-').str #não vai servir para nada\n",
    "    df['Year'], df['Month'], df['Day'] = date_split #não vai servir para nada\n",
    "    df[\"Volume\"] = df[\"Volume\"] / 10000 #não vai servir para nada\n",
    "    df.drop(df.columns[[0,3,5,6, 7,8,9]], axis=1, inplace=True) #vou só ficar com as colunas 1,2,4\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_GOOGL_stock_dataset():\n",
    "    stock_name = 'GOOGL'\n",
    "    return get_stock_data(stock_name,0, 'table.csv')\n",
    "\n",
    "def pre_processar_GOOGL_stock_dataset(df):\n",
    "    df['High'] = df['High'] / 100\n",
    "    df['Open'] = df['Open'] / 100\n",
    "    df['Close'] = df['Close'] / 100\n",
    "    return df\n",
    "\n",
    "# Visualizar os top registos da tabela\n",
    "def visualize_GOOGL():\n",
    "    df = load_GOOGL_stock_dataset()\n",
    "    print('### Antes do pré-processamento ###')\n",
    "    print(df.head()) #mostra só os primeiros 5 registos\n",
    "    df = pre_processar_GOOGL_stock_dataset(df)\n",
    "    print('### Após o pré-processamento ###')\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#função load_data do lstm.py configurada para aceitar qualquer número de parametros\n",
    "#o último atributo é que fica como label (resultado)\n",
    "#stock é um dataframe do pandas (uma especie de dicionario + matriz)\n",
    "#seq_len é o tamanho da janela a ser utilizada na serie temporal\n",
    "def load_data(df_dados, janela):\n",
    "    qt_atributos = len(df_dados.columns)\n",
    "    mat_dados = df_dados.as_matrix() #converter dataframe para matriz (lista com lista de cada registo)\n",
    "    tam_sequencia = janela + 1\n",
    "    res = []\n",
    "    for i in range(len(mat_dados) - tam_sequencia): #numero de registos - tamanho da sequencia\n",
    "         res.append(mat_dados[i: i + tam_sequencia])\n",
    "    res = np.array(res) #dá como resultado um np com uma lista de matrizes (janela deslizante ao longo da serie)\n",
    "    qt_casos_treino = int(round(0.9 * res.shape[0])) #90% passam a ser casos de treino\n",
    "    train = res[:qt_casos_treino, :]\n",
    "    x_train = train[:, :-1] #menos um registo pois o ultimo registo é o registo a seguir à janela\n",
    "    y_train = train[:, -1][:,-1] #para ir buscar o último atributo para a lista dos labels\n",
    "    x_test = res[qt_casos_treino:, :-1]\n",
    "    y_test = res[qt_casos_treino:, -1][:,-1]\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], qt_atributos))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], qt_atributos))\n",
    "    return [x_train, y_train, x_test, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etapa 2 - Definir a topologia da rede (arquitectura do modelo) e compilar '''\n",
    "def build_model2(janela):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=(janela, 3), return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(64, input_shape=(janela, 3), return_sequences=False))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(16, activation=\"relu\", kernel_initializer=\"uniform\"))\n",
    "    model.add(Dense(1, activation=\"linear\", kernel_initializer=\"uniform\"))\n",
    "    model.compile(loss='mse',optimizer='adam',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imprime um grafico com os valores de teste e com as correspondentes tabela de previsões\n",
    "def print_series_prediction(y_test,predic):\n",
    "    diff=[]\n",
    "    racio=[]\n",
    "    for i in range(len(y_test)): #para imprimir tabela de previsoes\n",
    "        racio.append( (y_test[i]/predic[i])-1)\n",
    "        diff.append( abs(y_test[i]- predic[i]))\n",
    "        print('valor: %f ---> Previsão: %f Diff: %f Racio: %f' % (y_test[i],predic[i], diff[i],racio[i]))\n",
    "    plt.plot(y_test,color='blue', label='y_test')\n",
    "    plt.plot(predic,color='red', label='prediction') #este deu uma linha em branco\n",
    "    plt.plot(diff,color='green', label='diff')\n",
    "    plt.plot(racio,color='yellow', label='racio')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_utilizando_GOOGL_data():\n",
    "    df = load_GOOGL_stock_dataset()\n",
    "    df = pre_processar_GOOGL_stock_dataset(df)\n",
    "    print(\"df\", df.shape)\n",
    "    janela = 22 #tamanho da Janela deslizante\n",
    "    X_train, y_train, X_test, y_test = load_data(df[::-1], janela)# o df[::-1] é o df por ordem inversa\n",
    "    print(\"X_train\", X_train.shape)\n",
    "    print(\"y_train\", y_train.shape)\n",
    "    print(\"X_test\", X_test.shape)\n",
    "    print(\"y_test\", y_test.shape)\n",
    "    #model = build_model(janela)\n",
    "    model = build_model2(janela)\n",
    "    #model.fit(X_train, y_train, batch_size=512, epochs=500, validation_split=0.1, verbose=1)\n",
    "    model.fit(X_train, y_train, batch_size=512, epochs=500, validation_split=0.1, verbose=1)\n",
    "    print_model(model,\"lstm_model.png\")\n",
    "    trainScore = model.evaluate(X_train, y_train, verbose=0)\n",
    "    print('Train Score: %.2f MSE (%.2f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\n",
    "    testScore = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test Score: %.2f MSE (%.2f RMSE)' % (testScore[0], math.sqrt(testScore[0])))\n",
    "    print(model.metrics_names)\n",
    "    p = model.predict(X_test)\n",
    "    predic = np.squeeze(np.asarray(p)) #para transformar uma matriz de uma coluna e n linhas em um np array de n elementos\n",
    "    print_series_prediction(y_test,predic)\n",
    "    # MSE- (Mean square error), RMSE- (root mean square error) - \n",
    "    # o significado de RMSE depende do range da label. para o mesmo range menor é melhor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model(model,fich):\n",
    " from keras.utils import plot_model\n",
    " plot_model(model, to_file=fich, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df (1350, 3)\n",
      "X_train (1194, 22, 3)\n",
      "y_train (1194,)\n",
      "X_test (133, 22, 3)\n",
      "y_test (133,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:7: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1074 samples, validate on 120 samples\n",
      "Epoch 1/500\n",
      "1074/1074 [==============================] - 2s 2ms/step - loss: 54.6070 - acc: 0.0000e+00 - val_loss: 57.5957 - val_acc: 0.0000e+00\n",
      "Epoch 2/500\n",
      "1074/1074 [==============================] - 0s 377us/step - loss: 54.1374 - acc: 0.0000e+00 - val_loss: 57.0660 - val_acc: 0.0000e+00\n",
      "Epoch 3/500\n",
      "1074/1074 [==============================] - 0s 361us/step - loss: 53.6271 - acc: 0.0000e+00 - val_loss: 56.4425 - val_acc: 0.0000e+00\n",
      "Epoch 4/500\n",
      "1074/1074 [==============================] - 0s 372us/step - loss: 53.0120 - acc: 0.0000e+00 - val_loss: 55.6395 - val_acc: 0.0000e+00\n",
      "Epoch 5/500\n",
      "1074/1074 [==============================] - 0s 368us/step - loss: 52.2080 - acc: 0.0000e+00 - val_loss: 54.5500 - val_acc: 0.0000e+00\n",
      "Epoch 6/500\n",
      "1074/1074 [==============================] - 0s 372us/step - loss: 51.1285 - acc: 0.0000e+00 - val_loss: 53.1437 - val_acc: 0.0000e+00\n",
      "Epoch 7/500\n",
      "1074/1074 [==============================] - 0s 352us/step - loss: 49.7615 - acc: 0.0000e+00 - val_loss: 51.5040 - val_acc: 0.0000e+00\n",
      "Epoch 8/500\n",
      "1074/1074 [==============================] - 0s 359us/step - loss: 48.2010 - acc: 0.0000e+00 - val_loss: 49.7338 - val_acc: 0.0000e+00\n",
      "Epoch 9/500\n",
      "1074/1074 [==============================] - 0s 349us/step - loss: 46.5129 - acc: 0.0000e+00 - val_loss: 47.8690 - val_acc: 0.0000e+00\n",
      "Epoch 10/500\n",
      "1074/1074 [==============================] - 0s 344us/step - loss: 44.7252 - acc: 0.0000e+00 - val_loss: 45.8112 - val_acc: 0.0000e+00\n",
      "Epoch 11/500\n",
      "1074/1074 [==============================] - 0s 393us/step - loss: 42.7533 - acc: 0.0000e+00 - val_loss: 43.5668 - val_acc: 0.0000e+00\n",
      "Epoch 12/500\n",
      "1074/1074 [==============================] - 0s 360us/step - loss: 40.6311 - acc: 0.0000e+00 - val_loss: 41.1758 - val_acc: 0.0000e+00\n",
      "Epoch 13/500\n",
      "1074/1074 [==============================] - 0s 370us/step - loss: 38.3640 - acc: 0.0000e+00 - val_loss: 38.6189 - val_acc: 0.0000e+00\n",
      "Epoch 14/500\n",
      "1074/1074 [==============================] - 0s 394us/step - loss: 35.9543 - acc: 0.0000e+00 - val_loss: 35.9090 - val_acc: 0.0000e+00\n",
      "Epoch 15/500\n",
      "1074/1074 [==============================] - 0s 378us/step - loss: 33.4141 - acc: 0.0000e+00 - val_loss: 33.1076 - val_acc: 0.0000e+00\n",
      "Epoch 16/500\n",
      "1074/1074 [==============================] - 0s 434us/step - loss: 30.8140 - acc: 0.0000e+00 - val_loss: 30.2407 - val_acc: 0.0000e+00\n",
      "Epoch 17/500\n",
      "1074/1074 [==============================] - 0s 388us/step - loss: 28.1488 - acc: 0.0000e+00 - val_loss: 27.3314 - val_acc: 0.0000e+00\n",
      "Epoch 18/500\n",
      "1074/1074 [==============================] - 0s 359us/step - loss: 25.4750 - acc: 0.0000e+00 - val_loss: 24.4075 - val_acc: 0.0000e+00\n",
      "Epoch 19/500\n",
      "1074/1074 [==============================] - 0s 350us/step - loss: 22.8098 - acc: 0.0000e+00 - val_loss: 21.5143 - val_acc: 0.0000e+00\n",
      "Epoch 20/500\n",
      "1074/1074 [==============================] - 0s 346us/step - loss: 20.1865 - acc: 0.0000e+00 - val_loss: 18.6976 - val_acc: 0.0000e+00\n",
      "Epoch 21/500\n",
      "1074/1074 [==============================] - 0s 366us/step - loss: 17.6564 - acc: 0.0000e+00 - val_loss: 15.9926 - val_acc: 0.0000e+00\n",
      "Epoch 22/500\n",
      "1074/1074 [==============================] - 0s 347us/step - loss: 15.2770 - acc: 0.0000e+00 - val_loss: 13.4437 - val_acc: 0.0000e+00\n",
      "Epoch 23/500\n",
      "1074/1074 [==============================] - 0s 350us/step - loss: 13.0401 - acc: 0.0000e+00 - val_loss: 11.1001 - val_acc: 0.0000e+00\n",
      "Epoch 24/500\n",
      "1074/1074 [==============================] - 0s 350us/step - loss: 11.0248 - acc: 0.0000e+00 - val_loss: 8.9722 - val_acc: 0.0000e+00\n",
      "Epoch 25/500\n",
      "1074/1074 [==============================] - 0s 350us/step - loss: 9.2323 - acc: 0.0000e+00 - val_loss: 7.0957 - val_acc: 0.0000e+00\n",
      "Epoch 26/500\n",
      "1074/1074 [==============================] - 0s 357us/step - loss: 7.7027 - acc: 0.0000e+00 - val_loss: 5.4707 - val_acc: 0.0000e+00\n",
      "Epoch 27/500\n",
      "1074/1074 [==============================] - 0s 346us/step - loss: 6.4054 - acc: 0.0000e+00 - val_loss: 4.1113 - val_acc: 0.0000e+00\n",
      "Epoch 28/500\n",
      "1074/1074 [==============================] - 0s 343us/step - loss: 5.3627 - acc: 0.0000e+00 - val_loss: 3.0087 - val_acc: 0.0000e+00\n",
      "Epoch 29/500\n",
      "1074/1074 [==============================] - 0s 352us/step - loss: 4.5517 - acc: 0.0000e+00 - val_loss: 2.1475 - val_acc: 0.0000e+00\n",
      "Epoch 30/500\n",
      "1074/1074 [==============================] - 0s 344us/step - loss: 3.9716 - acc: 0.0000e+00 - val_loss: 1.4927 - val_acc: 0.0000e+00\n",
      "Epoch 31/500\n",
      "1074/1074 [==============================] - 0s 351us/step - loss: 3.5668 - acc: 0.0000e+00 - val_loss: 1.0213 - val_acc: 0.0000e+00\n",
      "Epoch 32/500\n",
      "1074/1074 [==============================] - 0s 356us/step - loss: 3.3196 - acc: 0.0000e+00 - val_loss: 0.6984 - val_acc: 0.0000e+00\n",
      "Epoch 33/500\n",
      "1074/1074 [==============================] - 0s 355us/step - loss: 3.1854 - acc: 0.0000e+00 - val_loss: 0.4904 - val_acc: 0.0000e+00\n",
      "Epoch 34/500\n",
      "1074/1074 [==============================] - 0s 354us/step - loss: 3.1270 - acc: 0.0000e+00 - val_loss: 0.3660 - val_acc: 0.0000e+00\n",
      "Epoch 35/500\n",
      "1074/1074 [==============================] - 0s 357us/step - loss: 3.1081 - acc: 0.0000e+00 - val_loss: 0.2940 - val_acc: 0.0000e+00\n",
      "Epoch 36/500\n",
      "1074/1074 [==============================] - 0s 361us/step - loss: 3.1168 - acc: 0.0000e+00 - val_loss: 0.2507 - val_acc: 0.0000e+00\n",
      "Epoch 37/500\n",
      "1074/1074 [==============================] - 0s 342us/step - loss: 3.1268 - acc: 0.0000e+00 - val_loss: 0.2266 - val_acc: 0.0000e+00\n",
      "Epoch 38/500\n",
      "1074/1074 [==============================] - 1s 480us/step - loss: 3.1390 - acc: 0.0000e+00 - val_loss: 0.2175 - val_acc: 0.0000e+00\n",
      "Epoch 39/500\n",
      "1074/1074 [==============================] - 1s 483us/step - loss: 3.1443 - acc: 0.0000e+00 - val_loss: 0.2162 - val_acc: 0.0000e+00\n",
      "Epoch 40/500\n",
      "1074/1074 [==============================] - 1s 506us/step - loss: 3.1448 - acc: 0.0000e+00 - val_loss: 0.2200 - val_acc: 0.0000e+00\n",
      "Epoch 41/500\n",
      "1074/1074 [==============================] - 1s 484us/step - loss: 3.1417 - acc: 0.0000e+00 - val_loss: 0.2276 - val_acc: 0.0000e+00\n",
      "Epoch 42/500\n",
      "1074/1074 [==============================] - 1s 484us/step - loss: 3.1364 - acc: 0.0000e+00 - val_loss: 0.2335 - val_acc: 0.0000e+00\n",
      "Epoch 43/500\n",
      "1074/1074 [==============================] - 1s 491us/step - loss: 3.1325 - acc: 0.0000e+00 - val_loss: 0.2397 - val_acc: 0.0000e+00\n",
      "Epoch 44/500\n",
      "1074/1074 [==============================] - 1s 495us/step - loss: 3.1291 - acc: 0.0000e+00 - val_loss: 0.2485 - val_acc: 0.0000e+00\n",
      "Epoch 45/500\n",
      "1074/1074 [==============================] - 1s 495us/step - loss: 3.1252 - acc: 0.0000e+00 - val_loss: 0.2570 - val_acc: 0.0000e+00\n",
      "Epoch 46/500\n",
      "1074/1074 [==============================] - 1s 495us/step - loss: 3.1219 - acc: 0.0000e+00 - val_loss: 0.2622 - val_acc: 0.0000e+00\n",
      "Epoch 47/500\n",
      "1074/1074 [==============================] - 1s 518us/step - loss: 3.1200 - acc: 0.0000e+00 - val_loss: 0.2673 - val_acc: 0.0000e+00\n",
      "Epoch 48/500\n",
      "1074/1074 [==============================] - 1s 491us/step - loss: 3.1186 - acc: 0.0000e+00 - val_loss: 0.2706 - val_acc: 0.0000e+00\n",
      "Epoch 49/500\n",
      "1074/1074 [==============================] - 1s 495us/step - loss: 3.1176 - acc: 0.0000e+00 - val_loss: 0.2720 - val_acc: 0.0000e+00\n",
      "Epoch 50/500\n",
      "1074/1074 [==============================] - 1s 495us/step - loss: 3.1169 - acc: 0.0000e+00 - val_loss: 0.2776 - val_acc: 0.0000e+00\n",
      "Epoch 51/500\n",
      "1074/1074 [==============================] - 1s 489us/step - loss: 3.1156 - acc: 0.0000e+00 - val_loss: 0.2856 - val_acc: 0.0000e+00\n",
      "Epoch 52/500\n",
      "1074/1074 [==============================] - 1s 500us/step - loss: 3.1137 - acc: 0.0000e+00 - val_loss: 0.2939 - val_acc: 0.0000e+00\n",
      "Epoch 53/500\n",
      "1074/1074 [==============================] - 1s 510us/step - loss: 3.1119 - acc: 0.0000e+00 - val_loss: 0.3007 - val_acc: 0.0000e+00\n",
      "Epoch 54/500\n",
      "1074/1074 [==============================] - 1s 496us/step - loss: 3.1110 - acc: 0.0000e+00 - val_loss: 0.3090 - val_acc: 0.0000e+00\n",
      "Epoch 55/500\n",
      "1074/1074 [==============================] - 1s 487us/step - loss: 3.1102 - acc: 0.0000e+00 - val_loss: 0.3178 - val_acc: 0.0000e+00\n",
      "Epoch 56/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1074/1074 [==============================] - 1s 495us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3223 - val_acc: 0.0000e+00\n",
      "Epoch 57/500\n",
      "1074/1074 [==============================] - 1s 496us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3278 - val_acc: 0.0000e+00\n",
      "Epoch 58/500\n",
      "1074/1074 [==============================] - 1s 503us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3333 - val_acc: 0.0000e+00\n",
      "Epoch 59/500\n",
      "1074/1074 [==============================] - 1s 521us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3349 - val_acc: 0.0000e+00\n",
      "Epoch 60/500\n",
      "1074/1074 [==============================] - 1s 487us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3343 - val_acc: 0.0000e+00\n",
      "Epoch 61/500\n",
      "1074/1074 [==============================] - 1s 502us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3338 - val_acc: 0.0000e+00\n",
      "Epoch 62/500\n",
      "1074/1074 [==============================] - 1s 476us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3322 - val_acc: 0.0000e+00\n",
      "Epoch 63/500\n",
      "1074/1074 [==============================] - 1s 494us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3336 - val_acc: 0.0000e+00\n",
      "Epoch 64/500\n",
      "1074/1074 [==============================] - 1s 502us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3321 - val_acc: 0.0000e+00\n",
      "Epoch 65/500\n",
      "1074/1074 [==============================] - 1s 491us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3318 - val_acc: 0.0000e+00\n",
      "Epoch 66/500\n",
      "1074/1074 [==============================] - 1s 487us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3352 - val_acc: 0.0000e+00\n",
      "Epoch 67/500\n",
      "1074/1074 [==============================] - 1s 506us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3424 - val_acc: 0.0000e+00\n",
      "Epoch 68/500\n",
      "1074/1074 [==============================] - 1s 491us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3404 - val_acc: 0.0000e+00\n",
      "Epoch 69/500\n",
      "1074/1074 [==============================] - 1s 487us/step - loss: 3.1081 - acc: 0.0000e+00 - val_loss: 0.3315 - val_acc: 0.0000e+00\n",
      "Epoch 70/500\n",
      "1074/1074 [==============================] - 1s 491us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3237 - val_acc: 0.0000e+00\n",
      "Epoch 71/500\n",
      "1074/1074 [==============================] - 1s 491us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3117 - val_acc: 0.0000e+00\n",
      "Epoch 72/500\n",
      "1074/1074 [==============================] - 1s 516us/step - loss: 3.1109 - acc: 0.0000e+00 - val_loss: 0.3002 - val_acc: 0.0000e+00\n",
      "Epoch 73/500\n",
      "1074/1074 [==============================] - 1s 487us/step - loss: 3.1113 - acc: 0.0000e+00 - val_loss: 0.2955 - val_acc: 0.0000e+00\n",
      "Epoch 74/500\n",
      "1074/1074 [==============================] - 1s 487us/step - loss: 3.1123 - acc: 0.0000e+00 - val_loss: 0.2974 - val_acc: 0.0000e+00\n",
      "Epoch 75/500\n",
      "1074/1074 [==============================] - 1s 485us/step - loss: 3.1112 - acc: 0.0000e+00 - val_loss: 0.3062 - val_acc: 0.0000e+00\n",
      "Epoch 76/500\n",
      "1074/1074 [==============================] - 1s 492us/step - loss: 3.1101 - acc: 0.0000e+00 - val_loss: 0.3160 - val_acc: 0.0000e+00\n",
      "Epoch 77/500\n",
      "1074/1074 [==============================] - 1s 491us/step - loss: 3.1099 - acc: 0.0000e+00 - val_loss: 0.3296 - val_acc: 0.0000e+00\n",
      "Epoch 78/500\n",
      "1074/1074 [==============================] - 1s 506us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3348 - val_acc: 0.0000e+00\n",
      "Epoch 79/500\n",
      "1074/1074 [==============================] - 1s 491us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3384 - val_acc: 0.0000e+00\n",
      "Epoch 80/500\n",
      "1074/1074 [==============================] - 1s 491us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3441 - val_acc: 0.0000e+00\n",
      "Epoch 81/500\n",
      "1074/1074 [==============================] - 1s 502us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3484 - val_acc: 0.0000e+00\n",
      "Epoch 82/500\n",
      "1074/1074 [==============================] - 1s 507us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3475 - val_acc: 0.0000e+00\n",
      "Epoch 83/500\n",
      "1074/1074 [==============================] - 1s 490us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3441 - val_acc: 0.0000e+00\n",
      "Epoch 84/500\n",
      "1074/1074 [==============================] - 1s 503us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3381 - val_acc: 0.0000e+00\n",
      "Epoch 85/500\n",
      "1074/1074 [==============================] - 1s 506us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3319 - val_acc: 0.0000e+00\n",
      "Epoch 86/500\n",
      "1074/1074 [==============================] - 1s 498us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3268 - val_acc: 0.0000e+00\n",
      "Epoch 87/500\n",
      "1074/1074 [==============================] - 1s 487us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3209 - val_acc: 0.0000e+00\n",
      "Epoch 88/500\n",
      "1074/1074 [==============================] - 1s 516us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3194 - val_acc: 0.0000e+00\n",
      "Epoch 89/500\n",
      "1074/1074 [==============================] - 1s 500us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3180 - val_acc: 0.0000e+00\n",
      "Epoch 90/500\n",
      "1074/1074 [==============================] - 1s 491us/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3158 - val_acc: 0.0000e+00\n",
      "Epoch 91/500\n",
      "1074/1074 [==============================] - 1s 501us/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3180 - val_acc: 0.0000e+00\n",
      "Epoch 92/500\n",
      "1074/1074 [==============================] - 1s 492us/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3228 - val_acc: 0.0000e+00\n",
      "Epoch 93/500\n",
      "1074/1074 [==============================] - 1s 487us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3320 - val_acc: 0.0000e+00\n",
      "Epoch 94/500\n",
      "1074/1074 [==============================] - 1s 487us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3395 - val_acc: 0.0000e+00\n",
      "Epoch 95/500\n",
      "1074/1074 [==============================] - 1s 491us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3436 - val_acc: 0.0000e+00\n",
      "Epoch 96/500\n",
      "1074/1074 [==============================] - 1s 498us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3438 - val_acc: 0.0000e+00\n",
      "Epoch 97/500\n",
      "1074/1074 [==============================] - 1s 491us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3485 - val_acc: 0.0000e+00\n",
      "Epoch 98/500\n",
      "1074/1074 [==============================] - 1s 509us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3491 - val_acc: 0.0000e+00\n",
      "Epoch 99/500\n",
      "1074/1074 [==============================] - 1s 487us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3454 - val_acc: 0.0000e+00\n",
      "Epoch 100/500\n",
      "1074/1074 [==============================] - 1s 487us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3428 - val_acc: 0.0000e+00\n",
      "Epoch 101/500\n",
      "1074/1074 [==============================] - 1s 510us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3424 - val_acc: 0.0000e+00\n",
      "Epoch 102/500\n",
      "1074/1074 [==============================] - 1s 502us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3446 - val_acc: 0.0000e+00\n",
      "Epoch 103/500\n",
      "1074/1074 [==============================] - 1s 487us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3533 - val_acc: 0.0000e+00\n",
      "Epoch 104/500\n",
      "1074/1074 [==============================] - 1s 487us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3605 - val_acc: 0.0000e+00\n",
      "Epoch 105/500\n",
      "1074/1074 [==============================] - 1s 493us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3673 - val_acc: 0.0000e+00\n",
      "Epoch 106/500\n",
      "1074/1074 [==============================] - 1s 491us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3798 - val_acc: 0.0000e+00\n",
      "Epoch 107/500\n",
      "1074/1074 [==============================] - 1s 500us/step - loss: 3.1102 - acc: 0.0000e+00 - val_loss: 0.3879 - val_acc: 0.0000e+00\n",
      "Epoch 108/500\n",
      "1074/1074 [==============================] - 1s 493us/step - loss: 3.1105 - acc: 0.0000e+00 - val_loss: 0.3827 - val_acc: 0.0000e+00\n",
      "Epoch 109/500\n",
      "1074/1074 [==============================] - 1s 487us/step - loss: 3.1099 - acc: 0.0000e+00 - val_loss: 0.3772 - val_acc: 0.0000e+00\n",
      "Epoch 110/500\n",
      "1074/1074 [==============================] - 1s 488us/step - loss: 3.1096 - acc: 0.0000e+00 - val_loss: 0.3719 - val_acc: 0.0000e+00\n",
      "Epoch 111/500\n",
      "1074/1074 [==============================] - 1s 511us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3599 - val_acc: 0.0000e+00\n",
      "Epoch 112/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1074/1074 [==============================] - 1s 510us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3504 - val_acc: 0.0000e+00\n",
      "Epoch 113/500\n",
      "1074/1074 [==============================] - 1s 506us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3411 - val_acc: 0.0000e+00\n",
      "Epoch 114/500\n",
      "1074/1074 [==============================] - 1s 480us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3441 - val_acc: 0.0000e+00\n",
      "Epoch 115/500\n",
      "1074/1074 [==============================] - 1s 502us/step - loss: 3.1081 - acc: 0.0000e+00 - val_loss: 0.3503 - val_acc: 0.0000e+00\n",
      "Epoch 116/500\n",
      "1074/1074 [==============================] - 1s 491us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3526 - val_acc: 0.0000e+00\n",
      "Epoch 117/500\n",
      "1074/1074 [==============================] - 1s 512us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3470 - val_acc: 0.0000e+00\n",
      "Epoch 118/500\n",
      "1074/1074 [==============================] - 1s 487us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3362 - val_acc: 0.0000e+00\n",
      "Epoch 119/500\n",
      "1074/1074 [==============================] - 1s 517us/step - loss: 3.1079 - acc: 0.0000e+00 - val_loss: 0.3224 - val_acc: 0.0000e+00\n",
      "Epoch 120/500\n",
      "1074/1074 [==============================] - 1s 625us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3061 - val_acc: 0.0000e+00\n",
      "Epoch 121/500\n",
      "1074/1074 [==============================] - 1s 824us/step - loss: 3.1103 - acc: 0.0000e+00 - val_loss: 0.2989 - val_acc: 0.0000e+00\n",
      "Epoch 122/500\n",
      "1074/1074 [==============================] - 0s 371us/step - loss: 3.1114 - acc: 0.0000e+00 - val_loss: 0.2996 - val_acc: 0.0000e+00\n",
      "Epoch 123/500\n",
      "1074/1074 [==============================] - 0s 359us/step - loss: 3.1116 - acc: 0.0000e+00 - val_loss: 0.2981 - val_acc: 0.0000e+00\n",
      "Epoch 124/500\n",
      "1074/1074 [==============================] - 0s 363us/step - loss: 3.1115 - acc: 0.0000e+00 - val_loss: 0.3015 - val_acc: 0.0000e+00\n",
      "Epoch 125/500\n",
      "1074/1074 [==============================] - 0s 357us/step - loss: 3.1111 - acc: 0.0000e+00 - val_loss: 0.3078 - val_acc: 0.0000e+00\n",
      "Epoch 126/500\n",
      "1074/1074 [==============================] - 0s 364us/step - loss: 3.1102 - acc: 0.0000e+00 - val_loss: 0.3122 - val_acc: 0.0000e+00\n",
      "Epoch 127/500\n",
      "1074/1074 [==============================] - 0s 354us/step - loss: 3.1096 - acc: 0.0000e+00 - val_loss: 0.3188 - val_acc: 0.0000e+00\n",
      "Epoch 128/500\n",
      "1074/1074 [==============================] - 0s 350us/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3278 - val_acc: 0.0000e+00\n",
      "Epoch 129/500\n",
      "1074/1074 [==============================] - 0s 356us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3331 - val_acc: 0.0000e+00\n",
      "Epoch 130/500\n",
      "1074/1074 [==============================] - 0s 357us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3417 - val_acc: 0.0000e+00\n",
      "Epoch 131/500\n",
      "1074/1074 [==============================] - 0s 355us/step - loss: 3.1081 - acc: 0.0000e+00 - val_loss: 0.3489 - val_acc: 0.0000e+00\n",
      "Epoch 132/500\n",
      "1074/1074 [==============================] - 0s 354us/step - loss: 3.1080 - acc: 0.0000e+00 - val_loss: 0.3602 - val_acc: 0.0000e+00\n",
      "Epoch 133/500\n",
      "1074/1074 [==============================] - 0s 349us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3717 - val_acc: 0.0000e+00\n",
      "Epoch 134/500\n",
      "1074/1074 [==============================] - 0s 345us/step - loss: 3.1096 - acc: 0.0000e+00 - val_loss: 0.3729 - val_acc: 0.0000e+00\n",
      "Epoch 135/500\n",
      "1074/1074 [==============================] - 0s 346us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3594 - val_acc: 0.0000e+00\n",
      "Epoch 136/500\n",
      "1074/1074 [==============================] - 0s 345us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3460 - val_acc: 0.0000e+00\n",
      "Epoch 137/500\n",
      "1074/1074 [==============================] - 0s 370us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3352 - val_acc: 0.0000e+00\n",
      "Epoch 138/500\n",
      "1074/1074 [==============================] - 0s 386us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3262 - val_acc: 0.0000e+00\n",
      "Epoch 139/500\n",
      "1074/1074 [==============================] - 0s 390us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3178 - val_acc: 0.0000e+00\n",
      "Epoch 140/500\n",
      "1074/1074 [==============================] - 0s 403us/step - loss: 3.1100 - acc: 0.0000e+00 - val_loss: 0.3047 - val_acc: 0.0000e+00\n",
      "Epoch 141/500\n",
      "1074/1074 [==============================] - 0s 386us/step - loss: 3.1110 - acc: 0.0000e+00 - val_loss: 0.3044 - val_acc: 0.0000e+00\n",
      "Epoch 142/500\n",
      "1074/1074 [==============================] - 0s 368us/step - loss: 3.1102 - acc: 0.0000e+00 - val_loss: 0.3140 - val_acc: 0.0000e+00\n",
      "Epoch 143/500\n",
      "1074/1074 [==============================] - 0s 347us/step - loss: 3.1097 - acc: 0.0000e+00 - val_loss: 0.3185 - val_acc: 0.0000e+00\n",
      "Epoch 144/500\n",
      "1074/1074 [==============================] - 0s 348us/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3170 - val_acc: 0.0000e+00\n",
      "Epoch 145/500\n",
      "1074/1074 [==============================] - 0s 353us/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3162 - val_acc: 0.0000e+00\n",
      "Epoch 146/500\n",
      "1074/1074 [==============================] - 0s 351us/step - loss: 3.1096 - acc: 0.0000e+00 - val_loss: 0.3206 - val_acc: 0.0000e+00\n",
      "Epoch 147/500\n",
      "1074/1074 [==============================] - 0s 349us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3269 - val_acc: 0.0000e+00\n",
      "Epoch 148/500\n",
      "1074/1074 [==============================] - 0s 354us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3342 - val_acc: 0.0000e+00\n",
      "Epoch 149/500\n",
      "1074/1074 [==============================] - 0s 345us/step - loss: 3.1081 - acc: 0.0000e+00 - val_loss: 0.3460 - val_acc: 0.0000e+00\n",
      "Epoch 150/500\n",
      "1074/1074 [==============================] - 0s 345us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3584 - val_acc: 0.0000e+00\n",
      "Epoch 151/500\n",
      "1074/1074 [==============================] - 0s 350us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3553 - val_acc: 0.0000e+00\n",
      "Epoch 152/500\n",
      "1074/1074 [==============================] - 0s 352us/step - loss: 3.1081 - acc: 0.0000e+00 - val_loss: 0.3454 - val_acc: 0.0000e+00\n",
      "Epoch 153/500\n",
      "1074/1074 [==============================] - 0s 343us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3319 - val_acc: 0.0000e+00\n",
      "Epoch 154/500\n",
      "1074/1074 [==============================] - 0s 350us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3187 - val_acc: 0.0000e+00\n",
      "Epoch 155/500\n",
      "1074/1074 [==============================] - 0s 344us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3116 - val_acc: 0.0000e+00\n",
      "Epoch 156/500\n",
      "1074/1074 [==============================] - 0s 349us/step - loss: 3.1098 - acc: 0.0000e+00 - val_loss: 0.3066 - val_acc: 0.0000e+00\n",
      "Epoch 157/500\n",
      "1074/1074 [==============================] - 0s 356us/step - loss: 3.1105 - acc: 0.0000e+00 - val_loss: 0.3035 - val_acc: 0.0000e+00\n",
      "Epoch 158/500\n",
      "1074/1074 [==============================] - 0s 360us/step - loss: 3.1107 - acc: 0.0000e+00 - val_loss: 0.3076 - val_acc: 0.0000e+00\n",
      "Epoch 159/500\n",
      "1074/1074 [==============================] - 0s 345us/step - loss: 3.1102 - acc: 0.0000e+00 - val_loss: 0.3176 - val_acc: 0.0000e+00\n",
      "Epoch 160/500\n",
      "1074/1074 [==============================] - 0s 343us/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3271 - val_acc: 0.0000e+00\n",
      "Epoch 161/500\n",
      "1074/1074 [==============================] - 0s 340us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3342 - val_acc: 0.0000e+00\n",
      "Epoch 162/500\n",
      "1074/1074 [==============================] - 0s 347us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3342 - val_acc: 0.0000e+00\n",
      "Epoch 163/500\n",
      "1074/1074 [==============================] - 0s 347us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3358 - val_acc: 0.0000e+00\n",
      "Epoch 164/500\n",
      "1074/1074 [==============================] - 0s 346us/step - loss: 3.1080 - acc: 0.0000e+00 - val_loss: 0.3214 - val_acc: 0.0000e+00\n",
      "Epoch 165/500\n",
      "1074/1074 [==============================] - 0s 340us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3100 - val_acc: 0.0000e+00\n",
      "Epoch 166/500\n",
      "1074/1074 [==============================] - 0s 356us/step - loss: 3.1099 - acc: 0.0000e+00 - val_loss: 0.3162 - val_acc: 0.0000e+00\n",
      "Epoch 167/500\n",
      "1074/1074 [==============================] - 0s 344us/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.3362 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168/500\n",
      "1074/1074 [==============================] - 0s 346us/step - loss: 3.1080 - acc: 0.0000e+00 - val_loss: 0.3605 - val_acc: 0.0000e+00\n",
      "Epoch 169/500\n",
      "1074/1074 [==============================] - 0s 342us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3715 - val_acc: 0.0000e+00\n",
      "Epoch 170/500\n",
      "1074/1074 [==============================] - 0s 344us/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.3755 - val_acc: 0.0000e+00\n",
      "Epoch 171/500\n",
      "1074/1074 [==============================] - 0s 340us/step - loss: 3.1095 - acc: 0.0000e+00 - val_loss: 0.3756 - val_acc: 0.0000e+00\n",
      "Epoch 172/500\n",
      "1074/1074 [==============================] - 0s 341us/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3662 - val_acc: 0.0000e+00\n",
      "Epoch 173/500\n",
      "1074/1074 [==============================] - 0s 342us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3514 - val_acc: 0.0000e+00\n",
      "Epoch 174/500\n",
      "1074/1074 [==============================] - 0s 342us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3300 - val_acc: 0.0000e+00\n",
      "Epoch 175/500\n",
      "1074/1074 [==============================] - 0s 343us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3108 - val_acc: 0.0000e+00\n",
      "Epoch 176/500\n",
      "1074/1074 [==============================] - 0s 340us/step - loss: 3.1104 - acc: 0.0000e+00 - val_loss: 0.2936 - val_acc: 0.0000e+00\n",
      "Epoch 177/500\n",
      "1074/1074 [==============================] - 0s 341us/step - loss: 3.1128 - acc: 0.0000e+00 - val_loss: 0.2916 - val_acc: 0.0000e+00\n",
      "Epoch 178/500\n",
      "1074/1074 [==============================] - 0s 347us/step - loss: 3.1125 - acc: 0.0000e+00 - val_loss: 0.3049 - val_acc: 0.0000e+00\n",
      "Epoch 179/500\n",
      "1074/1074 [==============================] - 0s 344us/step - loss: 3.1102 - acc: 0.0000e+00 - val_loss: 0.3266 - val_acc: 0.0000e+00\n",
      "Epoch 180/500\n",
      "1074/1074 [==============================] - 0s 342us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3435 - val_acc: 0.0000e+00\n",
      "Epoch 181/500\n",
      "1074/1074 [==============================] - 0s 343us/step - loss: 3.1081 - acc: 0.0000e+00 - val_loss: 0.3643 - val_acc: 0.0000e+00\n",
      "Epoch 182/500\n",
      "1074/1074 [==============================] - 0s 348us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3861 - val_acc: 0.0000e+00\n",
      "Epoch 183/500\n",
      "1074/1074 [==============================] - 0s 342us/step - loss: 3.1109 - acc: 0.0000e+00 - val_loss: 0.3897 - val_acc: 0.0000e+00\n",
      "Epoch 184/500\n",
      "1074/1074 [==============================] - 0s 340us/step - loss: 3.1104 - acc: 0.0000e+00 - val_loss: 0.3757 - val_acc: 0.0000e+00\n",
      "Epoch 185/500\n",
      "1074/1074 [==============================] - 0s 341us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3569 - val_acc: 0.0000e+00\n",
      "Epoch 186/500\n",
      "1074/1074 [==============================] - 0s 339us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3345 - val_acc: 0.0000e+00\n",
      "Epoch 187/500\n",
      "1074/1074 [==============================] - 0s 338us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3188 - val_acc: 0.0000e+00\n",
      "Epoch 188/500\n",
      "1074/1074 [==============================] - 0s 348us/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.3071 - val_acc: 0.0000e+00\n",
      "Epoch 189/500\n",
      "1074/1074 [==============================] - 0s 342us/step - loss: 3.1106 - acc: 0.0000e+00 - val_loss: 0.3022 - val_acc: 0.0000e+00\n",
      "Epoch 190/500\n",
      "1074/1074 [==============================] - 0s 354us/step - loss: 3.1117 - acc: 0.0000e+00 - val_loss: 0.2980 - val_acc: 0.0000e+00\n",
      "Epoch 191/500\n",
      "1074/1074 [==============================] - 0s 344us/step - loss: 3.1116 - acc: 0.0000e+00 - val_loss: 0.2947 - val_acc: 0.0000e+00\n",
      "Epoch 192/500\n",
      "1074/1074 [==============================] - 0s 357us/step - loss: 3.1121 - acc: 0.0000e+00 - val_loss: 0.2904 - val_acc: 0.0000e+00\n",
      "Epoch 193/500\n",
      "1074/1074 [==============================] - 0s 351us/step - loss: 3.1129 - acc: 0.0000e+00 - val_loss: 0.2913 - val_acc: 0.0000e+00\n",
      "Epoch 194/500\n",
      "1074/1074 [==============================] - 0s 342us/step - loss: 3.1129 - acc: 0.0000e+00 - val_loss: 0.3028 - val_acc: 0.0000e+00\n",
      "Epoch 195/500\n",
      "1074/1074 [==============================] - 0s 346us/step - loss: 3.1096 - acc: 0.0000e+00 - val_loss: 0.3304 - val_acc: 0.0000e+00\n",
      "Epoch 196/500\n",
      "1074/1074 [==============================] - 0s 347us/step - loss: 3.1079 - acc: 0.0000e+00 - val_loss: 0.3549 - val_acc: 0.0000e+00\n",
      "Epoch 197/500\n",
      "1074/1074 [==============================] - 0s 345us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3662 - val_acc: 0.0000e+00\n",
      "Epoch 198/500\n",
      "1074/1074 [==============================] - 0s 343us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3680 - val_acc: 0.0000e+00\n",
      "Epoch 199/500\n",
      "1074/1074 [==============================] - 0s 358us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3661 - val_acc: 0.0000e+00\n",
      "Epoch 200/500\n",
      "1074/1074 [==============================] - 0s 343us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3611 - val_acc: 0.0000e+00\n",
      "Epoch 201/500\n",
      "1074/1074 [==============================] - 0s 345us/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3501 - val_acc: 0.0000e+00\n",
      "Epoch 202/500\n",
      "1074/1074 [==============================] - 0s 350us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3496 - val_acc: 0.0000e+00\n",
      "Epoch 203/500\n",
      "1074/1074 [==============================] - 0s 352us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3443 - val_acc: 0.0000e+00\n",
      "Epoch 204/500\n",
      "1074/1074 [==============================] - 0s 354us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3355 - val_acc: 0.0000e+00\n",
      "Epoch 205/500\n",
      "1074/1074 [==============================] - 0s 346us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3370 - val_acc: 0.0000e+00\n",
      "Epoch 206/500\n",
      "1074/1074 [==============================] - 0s 352us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3433 - val_acc: 0.0000e+00\n",
      "Epoch 207/500\n",
      "1074/1074 [==============================] - 0s 349us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3534 - val_acc: 0.0000e+00\n",
      "Epoch 208/500\n",
      "1074/1074 [==============================] - 0s 348us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3582 - val_acc: 0.0000e+00\n",
      "Epoch 209/500\n",
      "1074/1074 [==============================] - 0s 345us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3605 - val_acc: 0.0000e+00\n",
      "Epoch 210/500\n",
      "1074/1074 [==============================] - 0s 343us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3511 - val_acc: 0.0000e+00\n",
      "Epoch 211/500\n",
      "1074/1074 [==============================] - 0s 341us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3400 - val_acc: 0.0000e+00\n",
      "Epoch 212/500\n",
      "1074/1074 [==============================] - 0s 345us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3359 - val_acc: 0.0000e+00\n",
      "Epoch 213/500\n",
      "1074/1074 [==============================] - 0s 352us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3424 - val_acc: 0.0000e+00\n",
      "Epoch 214/500\n",
      "1074/1074 [==============================] - 0s 350us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3461 - val_acc: 0.0000e+00\n",
      "Epoch 215/500\n",
      "1074/1074 [==============================] - 0s 346us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3426 - val_acc: 0.0000e+00\n",
      "Epoch 216/500\n",
      "1074/1074 [==============================] - 0s 350us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3432 - val_acc: 0.0000e+00\n",
      "Epoch 217/500\n",
      "1074/1074 [==============================] - 0s 349us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3478 - val_acc: 0.0000e+00\n",
      "Epoch 218/500\n",
      "1074/1074 [==============================] - 0s 357us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3502 - val_acc: 0.0000e+00\n",
      "Epoch 219/500\n",
      "1074/1074 [==============================] - 0s 349us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3437 - val_acc: 0.0000e+00\n",
      "Epoch 220/500\n",
      "1074/1074 [==============================] - 0s 354us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3356 - val_acc: 0.0000e+00\n",
      "Epoch 221/500\n",
      "1074/1074 [==============================] - 0s 349us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3313 - val_acc: 0.0000e+00\n",
      "Epoch 222/500\n",
      "1074/1074 [==============================] - 0s 346us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3278 - val_acc: 0.0000e+00\n",
      "Epoch 223/500\n",
      "1074/1074 [==============================] - 0s 351us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3317 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 224/500\n",
      "1074/1074 [==============================] - 0s 345us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3271 - val_acc: 0.0000e+00\n",
      "Epoch 225/500\n",
      "1074/1074 [==============================] - 0s 358us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3275 - val_acc: 0.0000e+00\n",
      "Epoch 226/500\n",
      "1074/1074 [==============================] - 0s 350us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3393 - val_acc: 0.0000e+00\n",
      "Epoch 227/500\n",
      "1074/1074 [==============================] - 0s 349us/step - loss: 3.1074 - acc: 0.0000e+00 - val_loss: 0.3637 - val_acc: 0.0000e+00\n",
      "Epoch 228/500\n",
      "1074/1074 [==============================] - 0s 348us/step - loss: 3.1098 - acc: 0.0000e+00 - val_loss: 0.3874 - val_acc: 0.0000e+00\n",
      "Epoch 229/500\n",
      "1074/1074 [==============================] - 0s 346us/step - loss: 3.1108 - acc: 0.0000e+00 - val_loss: 0.3901 - val_acc: 0.0000e+00\n",
      "Epoch 230/500\n",
      "1074/1074 [==============================] - 0s 356us/step - loss: 3.1104 - acc: 0.0000e+00 - val_loss: 0.3743 - val_acc: 0.0000e+00\n",
      "Epoch 231/500\n",
      "1074/1074 [==============================] - 0s 348us/step - loss: 3.1095 - acc: 0.0000e+00 - val_loss: 0.3496 - val_acc: 0.0000e+00\n",
      "Epoch 232/500\n",
      "1074/1074 [==============================] - 0s 343us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3377 - val_acc: 0.0000e+00\n",
      "Epoch 233/500\n",
      "1074/1074 [==============================] - 0s 383us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3427 - val_acc: 0.0000e+00\n",
      "Epoch 234/500\n",
      "1074/1074 [==============================] - 0s 378us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3606 - val_acc: 0.0000e+00\n",
      "Epoch 235/500\n",
      "1074/1074 [==============================] - 0s 377us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3686 - val_acc: 0.0000e+00\n",
      "Epoch 236/500\n",
      "1074/1074 [==============================] - 0s 351us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3673 - val_acc: 0.0000e+00\n",
      "Epoch 237/500\n",
      "1074/1074 [==============================] - 0s 365us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3583 - val_acc: 0.0000e+00\n",
      "Epoch 238/500\n",
      "1074/1074 [==============================] - 0s 338us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3495 - val_acc: 0.0000e+00\n",
      "Epoch 239/500\n",
      "1074/1074 [==============================] - 0s 342us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3499 - val_acc: 0.0000e+00\n",
      "Epoch 240/500\n",
      "1074/1074 [==============================] - 0s 351us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3439 - val_acc: 0.0000e+00\n",
      "Epoch 241/500\n",
      "1074/1074 [==============================] - 0s 346us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3285 - val_acc: 0.0000e+00\n",
      "Epoch 242/500\n",
      "1074/1074 [==============================] - 0s 353us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3051 - val_acc: 0.0000e+00\n",
      "Epoch 243/500\n",
      "1074/1074 [==============================] - 0s 348us/step - loss: 3.1105 - acc: 0.0000e+00 - val_loss: 0.2860 - val_acc: 0.0000e+00\n",
      "Epoch 244/500\n",
      "1074/1074 [==============================] - 0s 353us/step - loss: 3.1152 - acc: 0.0000e+00 - val_loss: 0.2765 - val_acc: 0.0000e+00\n",
      "Epoch 245/500\n",
      "1074/1074 [==============================] - 0s 345us/step - loss: 3.1152 - acc: 0.0000e+00 - val_loss: 0.2902 - val_acc: 0.0000e+00\n",
      "Epoch 246/500\n",
      "1074/1074 [==============================] - 0s 347us/step - loss: 3.1120 - acc: 0.0000e+00 - val_loss: 0.3092 - val_acc: 0.0000e+00\n",
      "Epoch 247/500\n",
      "1074/1074 [==============================] - 0s 354us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3325 - val_acc: 0.0000e+00\n",
      "Epoch 248/500\n",
      "1074/1074 [==============================] - 0s 344us/step - loss: 3.1098 - acc: 0.0000e+00 - val_loss: 0.3685 - val_acc: 0.0000e+00\n",
      "Epoch 249/500\n",
      "1074/1074 [==============================] - 0s 348us/step - loss: 3.1097 - acc: 0.0000e+00 - val_loss: 0.3901 - val_acc: 0.0000e+00\n",
      "Epoch 250/500\n",
      "1074/1074 [==============================] - 0s 356us/step - loss: 3.1107 - acc: 0.0000e+00 - val_loss: 0.3987 - val_acc: 0.0000e+00\n",
      "Epoch 251/500\n",
      "1074/1074 [==============================] - 0s 347us/step - loss: 3.1117 - acc: 0.0000e+00 - val_loss: 0.4023 - val_acc: 0.0000e+00\n",
      "Epoch 252/500\n",
      "1074/1074 [==============================] - 0s 346us/step - loss: 3.1122 - acc: 0.0000e+00 - val_loss: 0.4030 - val_acc: 0.0000e+00\n",
      "Epoch 253/500\n",
      "1074/1074 [==============================] - 0s 347us/step - loss: 3.1122 - acc: 0.0000e+00 - val_loss: 0.3976 - val_acc: 0.0000e+00\n",
      "Epoch 254/500\n",
      "1074/1074 [==============================] - 0s 357us/step - loss: 3.1114 - acc: 0.0000e+00 - val_loss: 0.3864 - val_acc: 0.0000e+00\n",
      "Epoch 255/500\n",
      "1074/1074 [==============================] - 0s 360us/step - loss: 3.1101 - acc: 0.0000e+00 - val_loss: 0.3653 - val_acc: 0.0000e+00\n",
      "Epoch 256/500\n",
      "1074/1074 [==============================] - 0s 348us/step - loss: 3.1095 - acc: 0.0000e+00 - val_loss: 0.3431 - val_acc: 0.0000e+00\n",
      "Epoch 257/500\n",
      "1074/1074 [==============================] - 0s 347us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3250 - val_acc: 0.0000e+00\n",
      "Epoch 258/500\n",
      "1074/1074 [==============================] - 0s 356us/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.3249 - val_acc: 0.0000e+00\n",
      "Epoch 259/500\n",
      "1074/1074 [==============================] - 0s 344us/step - loss: 3.1080 - acc: 0.0000e+00 - val_loss: 0.3464 - val_acc: 0.0000e+00\n",
      "Epoch 260/500\n",
      "1074/1074 [==============================] - 0s 347us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3630 - val_acc: 0.0000e+00\n",
      "Epoch 261/500\n",
      "1074/1074 [==============================] - 0s 357us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3650 - val_acc: 0.0000e+00\n",
      "Epoch 262/500\n",
      "1074/1074 [==============================] - 0s 348us/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3798 - val_acc: 0.0000e+00\n",
      "Epoch 263/500\n",
      "1074/1074 [==============================] - 0s 347us/step - loss: 3.1099 - acc: 0.0000e+00 - val_loss: 0.3755 - val_acc: 0.0000e+00\n",
      "Epoch 264/500\n",
      "1074/1074 [==============================] - 0s 350us/step - loss: 3.1099 - acc: 0.0000e+00 - val_loss: 0.3691 - val_acc: 0.0000e+00\n",
      "Epoch 265/500\n",
      "1074/1074 [==============================] - 0s 347us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3742 - val_acc: 0.0000e+00\n",
      "Epoch 266/500\n",
      "1074/1074 [==============================] - 0s 347us/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3716 - val_acc: 0.0000e+00\n",
      "Epoch 267/500\n",
      "1074/1074 [==============================] - 0s 357us/step - loss: 3.1096 - acc: 0.0000e+00 - val_loss: 0.3598 - val_acc: 0.0000e+00\n",
      "Epoch 268/500\n",
      "1074/1074 [==============================] - 0s 344us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3549 - val_acc: 0.0000e+00\n",
      "Epoch 269/500\n",
      "1074/1074 [==============================] - 0s 349us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3604 - val_acc: 0.0000e+00\n",
      "Epoch 270/500\n",
      "1074/1074 [==============================] - 0s 348us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3708 - val_acc: 0.0000e+00\n",
      "Epoch 271/500\n",
      "1074/1074 [==============================] - 0s 357us/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.3642 - val_acc: 0.0000e+00\n",
      "Epoch 272/500\n",
      "1074/1074 [==============================] - 0s 348us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3313 - val_acc: 0.0000e+00\n",
      "Epoch 273/500\n",
      "1074/1074 [==============================] - 0s 352us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3037 - val_acc: 0.0000e+00\n",
      "Epoch 274/500\n",
      "1074/1074 [==============================] - 0s 346us/step - loss: 3.1112 - acc: 0.0000e+00 - val_loss: 0.2882 - val_acc: 0.0000e+00\n",
      "Epoch 275/500\n",
      "1074/1074 [==============================] - 0s 382us/step - loss: 3.1137 - acc: 0.0000e+00 - val_loss: 0.2853 - val_acc: 0.0000e+00\n",
      "Epoch 276/500\n",
      "1074/1074 [==============================] - 0s 377us/step - loss: 3.1136 - acc: 0.0000e+00 - val_loss: 0.2929 - val_acc: 0.0000e+00\n",
      "Epoch 277/500\n",
      "1074/1074 [==============================] - 0s 367us/step - loss: 3.1125 - acc: 0.0000e+00 - val_loss: 0.2997 - val_acc: 0.0000e+00\n",
      "Epoch 278/500\n",
      "1074/1074 [==============================] - 0s 348us/step - loss: 3.1113 - acc: 0.0000e+00 - val_loss: 0.3051 - val_acc: 0.0000e+00\n",
      "Epoch 279/500\n",
      "1074/1074 [==============================] - 0s 352us/step - loss: 3.1100 - acc: 0.0000e+00 - val_loss: 0.3224 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 280/500\n",
      "1074/1074 [==============================] - 0s 343us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3430 - val_acc: 0.0000e+00\n",
      "Epoch 281/500\n",
      "1074/1074 [==============================] - 0s 345us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3542 - val_acc: 0.0000e+00\n",
      "Epoch 282/500\n",
      "1074/1074 [==============================] - 0s 345us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3572 - val_acc: 0.0000e+00\n",
      "Epoch 283/500\n",
      "1074/1074 [==============================] - 0s 342us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3582 - val_acc: 0.0000e+00\n",
      "Epoch 284/500\n",
      "1074/1074 [==============================] - 0s 358us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3488 - val_acc: 0.0000e+00\n",
      "Epoch 285/500\n",
      "1074/1074 [==============================] - 0s 348us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3442 - val_acc: 0.0000e+00\n",
      "Epoch 286/500\n",
      "1074/1074 [==============================] - 0s 359us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3374 - val_acc: 0.0000e+00\n",
      "Epoch 287/500\n",
      "1074/1074 [==============================] - 0s 351us/step - loss: 3.1081 - acc: 0.0000e+00 - val_loss: 0.3204 - val_acc: 0.0000e+00\n",
      "Epoch 288/500\n",
      "1074/1074 [==============================] - 0s 345us/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.2954 - val_acc: 0.0000e+00\n",
      "Epoch 289/500\n",
      "1074/1074 [==============================] - 0s 347us/step - loss: 3.1128 - acc: 0.0000e+00 - val_loss: 0.2758 - val_acc: 0.0000e+00\n",
      "Epoch 290/500\n",
      "1074/1074 [==============================] - 0s 355us/step - loss: 3.1176 - acc: 0.0000e+00 - val_loss: 0.2716 - val_acc: 0.0000e+00\n",
      "Epoch 291/500\n",
      "1074/1074 [==============================] - 0s 349us/step - loss: 3.1166 - acc: 0.0000e+00 - val_loss: 0.2900 - val_acc: 0.0000e+00\n",
      "Epoch 292/500\n",
      "1074/1074 [==============================] - 0s 344us/step - loss: 3.1126 - acc: 0.0000e+00 - val_loss: 0.3253 - val_acc: 0.0000e+00\n",
      "Epoch 293/500\n",
      "1074/1074 [==============================] - 0s 360us/step - loss: 3.1098 - acc: 0.0000e+00 - val_loss: 0.3685 - val_acc: 0.0000e+00\n",
      "Epoch 294/500\n",
      "1074/1074 [==============================] - 0s 348us/step - loss: 3.1095 - acc: 0.0000e+00 - val_loss: 0.3931 - val_acc: 0.0000e+00\n",
      "Epoch 295/500\n",
      "1074/1074 [==============================] - 0s 344us/step - loss: 3.1113 - acc: 0.0000e+00 - val_loss: 0.4037 - val_acc: 0.0000e+00\n",
      "Epoch 296/500\n",
      "1074/1074 [==============================] - 0s 345us/step - loss: 3.1124 - acc: 0.0000e+00 - val_loss: 0.3870 - val_acc: 0.0000e+00\n",
      "Epoch 297/500\n",
      "1074/1074 [==============================] - 0s 347us/step - loss: 3.1099 - acc: 0.0000e+00 - val_loss: 0.3727 - val_acc: 0.0000e+00\n",
      "Epoch 298/500\n",
      "1074/1074 [==============================] - 0s 348us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3607 - val_acc: 0.0000e+00\n",
      "Epoch 299/500\n",
      "1074/1074 [==============================] - 0s 356us/step - loss: 3.1095 - acc: 0.0000e+00 - val_loss: 0.3412 - val_acc: 0.0000e+00\n",
      "Epoch 300/500\n",
      "1074/1074 [==============================] - 0s 345us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3343 - val_acc: 0.0000e+00\n",
      "Epoch 301/500\n",
      "1074/1074 [==============================] - 0s 344us/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.3193 - val_acc: 0.0000e+00\n",
      "Epoch 302/500\n",
      "1074/1074 [==============================] - 0s 348us/step - loss: 3.1097 - acc: 0.0000e+00 - val_loss: 0.3170 - val_acc: 0.0000e+00\n",
      "Epoch 303/500\n",
      "1074/1074 [==============================] - 0s 347us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3272 - val_acc: 0.0000e+00\n",
      "Epoch 304/500\n",
      "1074/1074 [==============================] - 0s 346us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3441 - val_acc: 0.0000e+00\n",
      "Epoch 305/500\n",
      "1074/1074 [==============================] - 0s 349us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3561 - val_acc: 0.0000e+00\n",
      "Epoch 306/500\n",
      "1074/1074 [==============================] - 0s 351us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3579 - val_acc: 0.0000e+00\n",
      "Epoch 307/500\n",
      "1074/1074 [==============================] - 0s 378us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3728 - val_acc: 0.0000e+00\n",
      "Epoch 308/500\n",
      "1074/1074 [==============================] - 0s 395us/step - loss: 3.1098 - acc: 0.0000e+00 - val_loss: 0.3800 - val_acc: 0.0000e+00\n",
      "Epoch 309/500\n",
      "1074/1074 [==============================] - 0s 398us/step - loss: 3.1104 - acc: 0.0000e+00 - val_loss: 0.3652 - val_acc: 0.0000e+00\n",
      "Epoch 310/500\n",
      "1074/1074 [==============================] - 0s 406us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3601 - val_acc: 0.0000e+00\n",
      "Epoch 311/500\n",
      "1074/1074 [==============================] - 0s 365us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3525 - val_acc: 0.0000e+00\n",
      "Epoch 312/500\n",
      "1074/1074 [==============================] - 0s 365us/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3386 - val_acc: 0.0000e+00\n",
      "Epoch 313/500\n",
      "1074/1074 [==============================] - 0s 345us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3330 - val_acc: 0.0000e+00\n",
      "Epoch 314/500\n",
      "1074/1074 [==============================] - 0s 348us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3401 - val_acc: 0.0000e+00\n",
      "Epoch 315/500\n",
      "1074/1074 [==============================] - 0s 359us/step - loss: 3.1080 - acc: 0.0000e+00 - val_loss: 0.3530 - val_acc: 0.0000e+00\n",
      "Epoch 316/500\n",
      "1074/1074 [==============================] - 0s 351us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3640 - val_acc: 0.0000e+00\n",
      "Epoch 317/500\n",
      "1074/1074 [==============================] - 0s 391us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3730 - val_acc: 0.0000e+00\n",
      "Epoch 318/500\n",
      "1074/1074 [==============================] - 0s 371us/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3738 - val_acc: 0.0000e+00\n",
      "Epoch 319/500\n",
      "1074/1074 [==============================] - 0s 386us/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3595 - val_acc: 0.0000e+00\n",
      "Epoch 320/500\n",
      "1074/1074 [==============================] - 0s 395us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3417 - val_acc: 0.0000e+00\n",
      "Epoch 321/500\n",
      "1074/1074 [==============================] - 0s 398us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3180 - val_acc: 0.0000e+00\n",
      "Epoch 322/500\n",
      "1074/1074 [==============================] - 0s 376us/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3043 - val_acc: 0.0000e+00\n",
      "Epoch 323/500\n",
      "1074/1074 [==============================] - 0s 353us/step - loss: 3.1109 - acc: 0.0000e+00 - val_loss: 0.2994 - val_acc: 0.0000e+00\n",
      "Epoch 324/500\n",
      "1074/1074 [==============================] - 0s 358us/step - loss: 3.1112 - acc: 0.0000e+00 - val_loss: 0.3141 - val_acc: 0.0000e+00\n",
      "Epoch 325/500\n",
      "1074/1074 [==============================] - 0s 396us/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3304 - val_acc: 0.0000e+00\n",
      "Epoch 326/500\n",
      "1074/1074 [==============================] - 0s 420us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3590 - val_acc: 0.0000e+00\n",
      "Epoch 327/500\n",
      "1074/1074 [==============================] - 0s 371us/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.3697 - val_acc: 0.0000e+00\n",
      "Epoch 328/500\n",
      "1074/1074 [==============================] - 0s 359us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3619 - val_acc: 0.0000e+00\n",
      "Epoch 329/500\n",
      "1074/1074 [==============================] - 0s 357us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3666 - val_acc: 0.0000e+00\n",
      "Epoch 330/500\n",
      "1074/1074 [==============================] - 0s 354us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3636 - val_acc: 0.0000e+00\n",
      "Epoch 331/500\n",
      "1074/1074 [==============================] - 0s 369us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3526 - val_acc: 0.0000e+00\n",
      "Epoch 332/500\n",
      "1074/1074 [==============================] - 0s 360us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3344 - val_acc: 0.0000e+00\n",
      "Epoch 333/500\n",
      "1074/1074 [==============================] - 0s 355us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3279 - val_acc: 0.0000e+00\n",
      "Epoch 334/500\n",
      "1074/1074 [==============================] - 0s 362us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3312 - val_acc: 0.0000e+00\n",
      "Epoch 335/500\n",
      "1074/1074 [==============================] - 0s 358us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3418 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 336/500\n",
      "1074/1074 [==============================] - 0s 354us/step - loss: 3.1078 - acc: 0.0000e+00 - val_loss: 0.3616 - val_acc: 0.0000e+00\n",
      "Epoch 337/500\n",
      "1074/1074 [==============================] - 0s 364us/step - loss: 3.1080 - acc: 0.0000e+00 - val_loss: 0.3903 - val_acc: 0.0000e+00\n",
      "Epoch 338/500\n",
      "1074/1074 [==============================] - 0s 359us/step - loss: 3.1121 - acc: 0.0000e+00 - val_loss: 0.4105 - val_acc: 0.0000e+00\n",
      "Epoch 339/500\n",
      "1074/1074 [==============================] - 0s 346us/step - loss: 3.1134 - acc: 0.0000e+00 - val_loss: 0.4128 - val_acc: 0.0000e+00\n",
      "Epoch 340/500\n",
      "1074/1074 [==============================] - 0s 348us/step - loss: 3.1136 - acc: 0.0000e+00 - val_loss: 0.4187 - val_acc: 0.0000e+00\n",
      "Epoch 341/500\n",
      "1074/1074 [==============================] - 0s 362us/step - loss: 3.1146 - acc: 0.0000e+00 - val_loss: 0.4174 - val_acc: 0.0000e+00\n",
      "Epoch 342/500\n",
      "1074/1074 [==============================] - 0s 345us/step - loss: 3.1145 - acc: 0.0000e+00 - val_loss: 0.4003 - val_acc: 0.0000e+00\n",
      "Epoch 343/500\n",
      "1074/1074 [==============================] - 0s 362us/step - loss: 3.1115 - acc: 0.0000e+00 - val_loss: 0.3809 - val_acc: 0.0000e+00\n",
      "Epoch 344/500\n",
      "1074/1074 [==============================] - 0s 348us/step - loss: 3.1102 - acc: 0.0000e+00 - val_loss: 0.3598 - val_acc: 0.0000e+00\n",
      "Epoch 345/500\n",
      "1074/1074 [==============================] - 0s 353us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3395 - val_acc: 0.0000e+00\n",
      "Epoch 346/500\n",
      "1074/1074 [==============================] - 0s 345us/step - loss: 3.1098 - acc: 0.0000e+00 - val_loss: 0.3058 - val_acc: 0.0000e+00\n",
      "Epoch 347/500\n",
      "1074/1074 [==============================] - 0s 349us/step - loss: 3.1106 - acc: 0.0000e+00 - val_loss: 0.2912 - val_acc: 0.0000e+00\n",
      "Epoch 348/500\n",
      "1074/1074 [==============================] - 0s 351us/step - loss: 3.1129 - acc: 0.0000e+00 - val_loss: 0.2916 - val_acc: 0.0000e+00\n",
      "Epoch 349/500\n",
      "1074/1074 [==============================] - 0s 356us/step - loss: 3.1125 - acc: 0.0000e+00 - val_loss: 0.3078 - val_acc: 0.0000e+00\n",
      "Epoch 350/500\n",
      "1074/1074 [==============================] - 0s 347us/step - loss: 3.1106 - acc: 0.0000e+00 - val_loss: 0.3244 - val_acc: 0.0000e+00\n",
      "Epoch 351/500\n",
      "1074/1074 [==============================] - 0s 351us/step - loss: 3.1087 - acc: 0.0000e+00 - val_loss: 0.3314 - val_acc: 0.0000e+00\n",
      "Epoch 352/500\n",
      "1074/1074 [==============================] - 0s 347us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3507 - val_acc: 0.0000e+00\n",
      "Epoch 353/500\n",
      "1074/1074 [==============================] - 0s 350us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3690 - val_acc: 0.0000e+00\n",
      "Epoch 354/500\n",
      "1074/1074 [==============================] - 0s 358us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3679 - val_acc: 0.0000e+00\n",
      "Epoch 355/500\n",
      "1074/1074 [==============================] - 0s 360us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3574 - val_acc: 0.0000e+00\n",
      "Epoch 356/500\n",
      "1074/1074 [==============================] - 0s 349us/step - loss: 3.1077 - acc: 0.0000e+00 - val_loss: 0.3326 - val_acc: 0.0000e+00\n",
      "Epoch 357/500\n",
      "1074/1074 [==============================] - 0s 349us/step - loss: 3.1094 - acc: 0.0000e+00 - val_loss: 0.3259 - val_acc: 0.0000e+00\n",
      "Epoch 358/500\n",
      "1074/1074 [==============================] - 0s 383us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3387 - val_acc: 0.0000e+00\n",
      "Epoch 359/500\n",
      "1074/1074 [==============================] - 0s 373us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3277 - val_acc: 0.0000e+00\n",
      "Epoch 360/500\n",
      "1074/1074 [==============================] - 0s 361us/step - loss: 3.1095 - acc: 0.0000e+00 - val_loss: 0.3072 - val_acc: 0.0000e+00\n",
      "Epoch 361/500\n",
      "1074/1074 [==============================] - 0s 353us/step - loss: 3.1104 - acc: 0.0000e+00 - val_loss: 0.2920 - val_acc: 0.0000e+00\n",
      "Epoch 362/500\n",
      "1074/1074 [==============================] - 0s 350us/step - loss: 3.1141 - acc: 0.0000e+00 - val_loss: 0.2733 - val_acc: 0.0000e+00\n",
      "Epoch 363/500\n",
      "1074/1074 [==============================] - 0s 357us/step - loss: 3.1170 - acc: 0.0000e+00 - val_loss: 0.2745 - val_acc: 0.0000e+00\n",
      "Epoch 364/500\n",
      "1074/1074 [==============================] - 0s 347us/step - loss: 3.1159 - acc: 0.0000e+00 - val_loss: 0.2957 - val_acc: 0.0000e+00\n",
      "Epoch 365/500\n",
      "1074/1074 [==============================] - 0s 345us/step - loss: 3.1113 - acc: 0.0000e+00 - val_loss: 0.3269 - val_acc: 0.0000e+00\n",
      "Epoch 366/500\n",
      "1074/1074 [==============================] - 0s 344us/step - loss: 3.1072 - acc: 0.0000e+00 - val_loss: 0.3568 - val_acc: 0.0000e+00\n",
      "Epoch 367/500\n",
      "1074/1074 [==============================] - 0s 364us/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.3836 - val_acc: 0.0000e+00\n",
      "Epoch 368/500\n",
      "1074/1074 [==============================] - 0s 399us/step - loss: 3.1105 - acc: 0.0000e+00 - val_loss: 0.3883 - val_acc: 0.0000e+00\n",
      "Epoch 369/500\n",
      "1074/1074 [==============================] - 0s 369us/step - loss: 3.1106 - acc: 0.0000e+00 - val_loss: 0.3996 - val_acc: 0.0000e+00\n",
      "Epoch 370/500\n",
      "1074/1074 [==============================] - 0s 354us/step - loss: 3.1119 - acc: 0.0000e+00 - val_loss: 0.3961 - val_acc: 0.0000e+00\n",
      "Epoch 371/500\n",
      "1074/1074 [==============================] - 0s 356us/step - loss: 3.1114 - acc: 0.0000e+00 - val_loss: 0.3871 - val_acc: 0.0000e+00\n",
      "Epoch 372/500\n",
      "1074/1074 [==============================] - 0s 362us/step - loss: 3.1100 - acc: 0.0000e+00 - val_loss: 0.3710 - val_acc: 0.0000e+00\n",
      "Epoch 373/500\n",
      "1074/1074 [==============================] - 0s 367us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3631 - val_acc: 0.0000e+00\n",
      "Epoch 374/500\n",
      "1074/1074 [==============================] - 0s 359us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3560 - val_acc: 0.0000e+00\n",
      "Epoch 375/500\n",
      "1074/1074 [==============================] - 0s 345us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3454 - val_acc: 0.0000e+00\n",
      "Epoch 376/500\n",
      "1074/1074 [==============================] - 0s 353us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3376 - val_acc: 0.0000e+00\n",
      "Epoch 377/500\n",
      "1074/1074 [==============================] - 0s 352us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3340 - val_acc: 0.0000e+00\n",
      "Epoch 378/500\n",
      "1074/1074 [==============================] - 0s 350us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3245 - val_acc: 0.0000e+00\n",
      "Epoch 379/500\n",
      "1074/1074 [==============================] - 0s 355us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3210 - val_acc: 0.0000e+00\n",
      "Epoch 380/500\n",
      "1074/1074 [==============================] - 0s 351us/step - loss: 3.1091 - acc: 0.0000e+00 - val_loss: 0.3152 - val_acc: 0.0000e+00\n",
      "Epoch 381/500\n",
      "1074/1074 [==============================] - 0s 353us/step - loss: 3.1098 - acc: 0.0000e+00 - val_loss: 0.3073 - val_acc: 0.0000e+00\n",
      "Epoch 382/500\n",
      "1074/1074 [==============================] - 0s 348us/step - loss: 3.1102 - acc: 0.0000e+00 - val_loss: 0.3177 - val_acc: 0.0000e+00\n",
      "Epoch 383/500\n",
      "1074/1074 [==============================] - 0s 349us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3466 - val_acc: 0.0000e+00\n",
      "Epoch 384/500\n",
      "1074/1074 [==============================] - 0s 353us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3739 - val_acc: 0.0000e+00\n",
      "Epoch 385/500\n",
      "1074/1074 [==============================] - 0s 347us/step - loss: 3.1097 - acc: 0.0000e+00 - val_loss: 0.3944 - val_acc: 0.0000e+00\n",
      "Epoch 386/500\n",
      "1074/1074 [==============================] - 0s 357us/step - loss: 3.1119 - acc: 0.0000e+00 - val_loss: 0.3929 - val_acc: 0.0000e+00\n",
      "Epoch 387/500\n",
      "1074/1074 [==============================] - 0s 349us/step - loss: 3.1126 - acc: 0.0000e+00 - val_loss: 0.3670 - val_acc: 0.0000e+00\n",
      "Epoch 388/500\n",
      "1074/1074 [==============================] - 0s 351us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3624 - val_acc: 0.0000e+00\n",
      "Epoch 389/500\n",
      "1074/1074 [==============================] - 0s 347us/step - loss: 3.1090 - acc: 0.0000e+00 - val_loss: 0.3509 - val_acc: 0.0000e+00\n",
      "Epoch 390/500\n",
      "1074/1074 [==============================] - 0s 350us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3438 - val_acc: 0.0000e+00\n",
      "Epoch 391/500\n",
      "1074/1074 [==============================] - 0s 357us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3400 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 392/500\n",
      "1074/1074 [==============================] - 0s 353us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3482 - val_acc: 0.0000e+00\n",
      "Epoch 393/500\n",
      "1074/1074 [==============================] - 0s 348us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3565 - val_acc: 0.0000e+00\n",
      "Epoch 394/500\n",
      "1074/1074 [==============================] - 0s 346us/step - loss: 3.1101 - acc: 0.0000e+00 - val_loss: 0.3725 - val_acc: 0.0000e+00\n",
      "Epoch 395/500\n",
      "1074/1074 [==============================] - 0s 361us/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.3692 - val_acc: 0.0000e+00\n",
      "Epoch 396/500\n",
      "1074/1074 [==============================] - 0s 347us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3601 - val_acc: 0.0000e+00\n",
      "Epoch 397/500\n",
      "1074/1074 [==============================] - 0s 353us/step - loss: 3.1092 - acc: 0.0000e+00 - val_loss: 0.3367 - val_acc: 0.0000e+00\n",
      "Epoch 398/500\n",
      "1074/1074 [==============================] - 0s 346us/step - loss: 3.1074 - acc: 0.0000e+00 - val_loss: 0.3089 - val_acc: 0.0000e+00\n",
      "Epoch 399/500\n",
      "1074/1074 [==============================] - 0s 368us/step - loss: 3.1119 - acc: 0.0000e+00 - val_loss: 0.2847 - val_acc: 0.0000e+00\n",
      "Epoch 400/500\n",
      "1074/1074 [==============================] - 0s 402us/step - loss: 3.1147 - acc: 0.0000e+00 - val_loss: 0.2806 - val_acc: 0.0000e+00\n",
      "Epoch 401/500\n",
      "1074/1074 [==============================] - 0s 367us/step - loss: 3.1147 - acc: 0.0000e+00 - val_loss: 0.2901 - val_acc: 0.0000e+00\n",
      "Epoch 402/500\n",
      "1074/1074 [==============================] - 0s 367us/step - loss: 3.1126 - acc: 0.0000e+00 - val_loss: 0.3024 - val_acc: 0.0000e+00\n",
      "Epoch 403/500\n",
      "1074/1074 [==============================] - 0s 352us/step - loss: 3.1118 - acc: 0.0000e+00 - val_loss: 0.3261 - val_acc: 0.0000e+00\n",
      "Epoch 404/500\n",
      "1074/1074 [==============================] - 0s 356us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3359 - val_acc: 0.0000e+00\n",
      "Epoch 405/500\n",
      "1074/1074 [==============================] - 0s 365us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3248 - val_acc: 0.0000e+00\n",
      "Epoch 406/500\n",
      "1074/1074 [==============================] - 0s 396us/step - loss: 3.1106 - acc: 0.0000e+00 - val_loss: 0.3029 - val_acc: 0.0000e+00\n",
      "Epoch 407/500\n",
      "1074/1074 [==============================] - 0s 394us/step - loss: 3.1109 - acc: 0.0000e+00 - val_loss: 0.3024 - val_acc: 0.0000e+00\n",
      "Epoch 408/500\n",
      "1074/1074 [==============================] - 0s 367us/step - loss: 3.1110 - acc: 0.0000e+00 - val_loss: 0.3036 - val_acc: 0.0000e+00\n",
      "Epoch 409/500\n",
      "1074/1074 [==============================] - 0s 352us/step - loss: 3.1110 - acc: 0.0000e+00 - val_loss: 0.3221 - val_acc: 0.0000e+00\n",
      "Epoch 410/500\n",
      "1074/1074 [==============================] - 0s 350us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3334 - val_acc: 0.0000e+00\n",
      "Epoch 411/500\n",
      "1074/1074 [==============================] - 0s 359us/step - loss: 3.1084 - acc: 0.0000e+00 - val_loss: 0.3339 - val_acc: 0.0000e+00\n",
      "Epoch 412/500\n",
      "1074/1074 [==============================] - 0s 349us/step - loss: 3.1086 - acc: 0.0000e+00 - val_loss: 0.3420 - val_acc: 0.0000e+00\n",
      "Epoch 413/500\n",
      "1074/1074 [==============================] - 0s 356us/step - loss: 3.1093 - acc: 0.0000e+00 - val_loss: 0.3444 - val_acc: 0.0000e+00\n",
      "Epoch 414/500\n",
      "1074/1074 [==============================] - 0s 366us/step - loss: 3.1079 - acc: 0.0000e+00 - val_loss: 0.3307 - val_acc: 0.0000e+00\n",
      "Epoch 415/500\n",
      "1074/1074 [==============================] - 0s 345us/step - loss: 3.1085 - acc: 0.0000e+00 - val_loss: 0.3302 - val_acc: 0.0000e+00\n",
      "Epoch 416/500\n",
      "1074/1074 [==============================] - 0s 358us/step - loss: 3.1088 - acc: 0.0000e+00 - val_loss: 0.3324 - val_acc: 0.0000e+00\n",
      "Epoch 417/500\n",
      "1074/1074 [==============================] - 0s 367us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3441 - val_acc: 0.0000e+00\n",
      "Epoch 418/500\n",
      "1074/1074 [==============================] - 0s 362us/step - loss: 3.1082 - acc: 0.0000e+00 - val_loss: 0.3532 - val_acc: 0.0000e+00\n",
      "Epoch 419/500\n",
      "1074/1074 [==============================] - 0s 354us/step - loss: 3.1083 - acc: 0.0000e+00 - val_loss: 0.3678 - val_acc: 0.0000e+00\n",
      "Epoch 420/500\n",
      "1074/1074 [==============================] - 0s 351us/step - loss: 3.1089 - acc: 0.0000e+00 - val_loss: 0.3832 - val_acc: 0.0000e+00\n",
      "Epoch 421/500\n",
      "1074/1074 [==============================] - 0s 360us/step - loss: 3.1103 - acc: 0.0000e+00 - val_loss: 0.3980 - val_acc: 0.0000e+00\n",
      "Epoch 422/500\n",
      "1074/1074 [==============================] - 0s 352us/step - loss: 3.1125 - acc: 0.0000e+00 - val_loss: 0.4158 - val_acc: 0.0000e+00\n",
      "Epoch 423/500\n",
      "1074/1074 [==============================] - 0s 353us/step - loss: 3.1146 - acc: 0.0000e+00 - val_loss: 0.4315 - val_acc: 0.0000e+00\n",
      "Epoch 424/500\n",
      "1074/1074 [==============================] - 0s 388us/step - loss: 3.1160 - acc: 0.0000e+00 - val_loss: 0.4263 - val_acc: 0.0000e+00\n",
      "Epoch 425/500\n",
      "1074/1074 [==============================] - 0s 372us/step - loss: 3.1146 - acc: 0.0000e+00 - val_loss: 0.3926 - val_acc: 0.0000e+00\n",
      "Epoch 426/500\n",
      "1074/1074 [==============================] - 0s 399us/step - loss: 3.1081 - acc: 0.0000e+00 - val_loss: 0.3332 - val_acc: 0.0000e+00\n",
      "Epoch 427/500\n",
      "1074/1074 [==============================] - 0s 405us/step - loss: 3.1077 - acc: 0.0000e+00 - val_loss: 0.3012 - val_acc: 0.0000e+00\n",
      "Epoch 428/500\n",
      "1074/1074 [==============================] - 0s 402us/step - loss: 3.1141 - acc: 0.0000e+00 - val_loss: 0.2812 - val_acc: 0.0000e+00\n",
      "Epoch 429/500\n",
      "1074/1074 [==============================] - 0s 366us/step - loss: 3.1130 - acc: 0.0000e+00 - val_loss: 0.2979 - val_acc: 0.0000e+00\n",
      "Epoch 430/500\n",
      "1074/1074 [==============================] - 0s 360us/step - loss: 3.1038 - acc: 0.0000e+00 - val_loss: 0.3385 - val_acc: 0.0000e+00\n",
      "Epoch 431/500\n",
      "1074/1074 [==============================] - 0s 366us/step - loss: 3.1055 - acc: 0.0000e+00 - val_loss: 0.3773 - val_acc: 0.0000e+00\n",
      "Epoch 432/500\n",
      "1074/1074 [==============================] - 0s 370us/step - loss: 3.1041 - acc: 0.0000e+00 - val_loss: 0.3747 - val_acc: 0.0000e+00\n",
      "Epoch 433/500\n",
      "1074/1074 [==============================] - 0s 374us/step - loss: 3.0985 - acc: 0.0000e+00 - val_loss: 0.3734 - val_acc: 0.0000e+00\n",
      "Epoch 434/500\n",
      "1074/1074 [==============================] - 0s 371us/step - loss: 3.0991 - acc: 0.0000e+00 - val_loss: 0.3783 - val_acc: 0.0000e+00\n",
      "Epoch 435/500\n",
      "1074/1074 [==============================] - 0s 383us/step - loss: 3.0983 - acc: 0.0000e+00 - val_loss: 0.3805 - val_acc: 0.0000e+00\n",
      "Epoch 436/500\n",
      "1074/1074 [==============================] - 0s 362us/step - loss: 3.0938 - acc: 0.0000e+00 - val_loss: 0.3597 - val_acc: 0.0000e+00\n",
      "Epoch 437/500\n",
      "1074/1074 [==============================] - 0s 363us/step - loss: 3.0891 - acc: 0.0000e+00 - val_loss: 0.3225 - val_acc: 0.0000e+00\n",
      "Epoch 438/500\n",
      "1074/1074 [==============================] - 0s 373us/step - loss: 3.0876 - acc: 0.0000e+00 - val_loss: 0.2821 - val_acc: 0.0000e+00\n",
      "Epoch 439/500\n",
      "1074/1074 [==============================] - 0s 389us/step - loss: 3.0926 - acc: 0.0000e+00 - val_loss: 0.2576 - val_acc: 0.0000e+00\n",
      "Epoch 440/500\n",
      "1074/1074 [==============================] - 0s 417us/step - loss: 3.0936 - acc: 0.0000e+00 - val_loss: 0.2631 - val_acc: 0.0000e+00\n",
      "Epoch 441/500\n",
      "1074/1074 [==============================] - 0s 396us/step - loss: 3.0883 - acc: 0.0000e+00 - val_loss: 0.2688 - val_acc: 0.0000e+00\n",
      "Epoch 442/500\n",
      "1074/1074 [==============================] - 0s 382us/step - loss: 3.0542 - acc: 0.0000e+00 - val_loss: 0.4489 - val_acc: 0.0000e+00\n",
      "Epoch 443/500\n",
      "1074/1074 [==============================] - 0s 373us/step - loss: 3.0271 - acc: 0.0000e+00 - val_loss: 0.3215 - val_acc: 0.0000e+00\n",
      "Epoch 444/500\n",
      "1074/1074 [==============================] - 0s 371us/step - loss: 2.9899 - acc: 0.0000e+00 - val_loss: 0.3538 - val_acc: 0.0000e+00\n",
      "Epoch 445/500\n",
      "1074/1074 [==============================] - 0s 356us/step - loss: 2.8775 - acc: 0.0000e+00 - val_loss: 0.4663 - val_acc: 0.0000e+00\n",
      "Epoch 446/500\n",
      "1074/1074 [==============================] - 0s 364us/step - loss: 2.8465 - acc: 0.0000e+00 - val_loss: 0.2922 - val_acc: 0.0000e+00\n",
      "Epoch 447/500\n",
      "1074/1074 [==============================] - 0s 366us/step - loss: 2.7986 - acc: 0.0000e+00 - val_loss: 0.2246 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 448/500\n",
      "1074/1074 [==============================] - 0s 372us/step - loss: 2.7836 - acc: 0.0000e+00 - val_loss: 0.2435 - val_acc: 0.0000e+00\n",
      "Epoch 449/500\n",
      "1074/1074 [==============================] - 0s 380us/step - loss: 2.7717 - acc: 0.0000e+00 - val_loss: 0.2106 - val_acc: 0.0000e+00\n",
      "Epoch 450/500\n",
      "1074/1074 [==============================] - 0s 386us/step - loss: 2.7520 - acc: 0.0000e+00 - val_loss: 0.2110 - val_acc: 0.0000e+00\n",
      "Epoch 451/500\n",
      "1074/1074 [==============================] - 0s 368us/step - loss: 2.7305 - acc: 0.0000e+00 - val_loss: 0.2174 - val_acc: 0.0000e+00\n",
      "Epoch 452/500\n",
      "1074/1074 [==============================] - 0s 369us/step - loss: 2.7132 - acc: 0.0000e+00 - val_loss: 0.2310 - val_acc: 0.0000e+00\n",
      "Epoch 453/500\n",
      "1074/1074 [==============================] - 0s 371us/step - loss: 2.6957 - acc: 0.0000e+00 - val_loss: 0.2249 - val_acc: 0.0000e+00\n",
      "Epoch 454/500\n",
      "1074/1074 [==============================] - 0s 372us/step - loss: 2.6837 - acc: 0.0000e+00 - val_loss: 0.2222 - val_acc: 0.0000e+00\n",
      "Epoch 455/500\n",
      "1074/1074 [==============================] - 0s 370us/step - loss: 2.6645 - acc: 0.0000e+00 - val_loss: 0.2685 - val_acc: 0.0000e+00\n",
      "Epoch 456/500\n",
      "1074/1074 [==============================] - 0s 380us/step - loss: 2.6490 - acc: 0.0000e+00 - val_loss: 0.2012 - val_acc: 0.0000e+00\n",
      "Epoch 457/500\n",
      "1074/1074 [==============================] - 0s 371us/step - loss: 2.6230 - acc: 0.0000e+00 - val_loss: 0.2679 - val_acc: 0.0000e+00\n",
      "Epoch 458/500\n",
      "1074/1074 [==============================] - 0s 386us/step - loss: 2.6200 - acc: 0.0000e+00 - val_loss: 0.1861 - val_acc: 0.0000e+00\n",
      "Epoch 459/500\n",
      "1074/1074 [==============================] - 0s 378us/step - loss: 2.5885 - acc: 0.0000e+00 - val_loss: 0.1983 - val_acc: 0.0000e+00\n",
      "Epoch 460/500\n",
      "1074/1074 [==============================] - 0s 360us/step - loss: 2.5706 - acc: 0.0000e+00 - val_loss: 0.1792 - val_acc: 0.0000e+00\n",
      "Epoch 461/500\n",
      "1074/1074 [==============================] - 0s 365us/step - loss: 2.5542 - acc: 0.0000e+00 - val_loss: 0.1789 - val_acc: 0.0000e+00\n",
      "Epoch 462/500\n",
      "1074/1074 [==============================] - 0s 383us/step - loss: 2.5393 - acc: 0.0000e+00 - val_loss: 0.1944 - val_acc: 0.0000e+00\n",
      "Epoch 463/500\n",
      "1074/1074 [==============================] - 0s 365us/step - loss: 2.5171 - acc: 0.0000e+00 - val_loss: 0.1850 - val_acc: 0.0000e+00\n",
      "Epoch 464/500\n",
      "1074/1074 [==============================] - 0s 372us/step - loss: 2.5014 - acc: 0.0000e+00 - val_loss: 0.1858 - val_acc: 0.0000e+00\n",
      "Epoch 465/500\n",
      "1074/1074 [==============================] - 0s 377us/step - loss: 2.4853 - acc: 0.0000e+00 - val_loss: 0.1850 - val_acc: 0.0000e+00\n",
      "Epoch 466/500\n",
      "1074/1074 [==============================] - 0s 370us/step - loss: 2.4730 - acc: 0.0000e+00 - val_loss: 0.2028 - val_acc: 0.0000e+00\n",
      "Epoch 467/500\n",
      "1074/1074 [==============================] - 0s 371us/step - loss: 2.4571 - acc: 0.0000e+00 - val_loss: 0.1633 - val_acc: 0.0000e+00\n",
      "Epoch 468/500\n",
      "1074/1074 [==============================] - 0s 376us/step - loss: 2.4379 - acc: 0.0000e+00 - val_loss: 0.1757 - val_acc: 0.0000e+00\n",
      "Epoch 469/500\n",
      "1074/1074 [==============================] - 0s 374us/step - loss: 2.4315 - acc: 0.0000e+00 - val_loss: 0.1595 - val_acc: 0.0000e+00\n",
      "Epoch 470/500\n",
      "1074/1074 [==============================] - 0s 368us/step - loss: 2.4150 - acc: 0.0000e+00 - val_loss: 0.1746 - val_acc: 0.0000e+00\n",
      "Epoch 471/500\n",
      "1074/1074 [==============================] - 0s 380us/step - loss: 2.4036 - acc: 0.0000e+00 - val_loss: 0.1613 - val_acc: 0.0000e+00\n",
      "Epoch 472/500\n",
      "1074/1074 [==============================] - 0s 369us/step - loss: 2.3872 - acc: 0.0000e+00 - val_loss: 0.1640 - val_acc: 0.0000e+00\n",
      "Epoch 473/500\n",
      "1074/1074 [==============================] - 0s 370us/step - loss: 2.3883 - acc: 0.0000e+00 - val_loss: 0.2208 - val_acc: 0.0000e+00\n",
      "Epoch 474/500\n",
      "1074/1074 [==============================] - 0s 378us/step - loss: 2.3567 - acc: 0.0000e+00 - val_loss: 0.1666 - val_acc: 0.0000e+00\n",
      "Epoch 475/500\n",
      "1074/1074 [==============================] - 0s 373us/step - loss: 2.3399 - acc: 0.0000e+00 - val_loss: 0.2696 - val_acc: 0.0000e+00\n",
      "Epoch 476/500\n",
      "1074/1074 [==============================] - 0s 364us/step - loss: 2.3411 - acc: 0.0000e+00 - val_loss: 0.1613 - val_acc: 0.0000e+00\n",
      "Epoch 477/500\n",
      "1074/1074 [==============================] - 0s 366us/step - loss: 2.3419 - acc: 0.0000e+00 - val_loss: 0.3144 - val_acc: 0.0000e+00\n",
      "Epoch 478/500\n",
      "1074/1074 [==============================] - 0s 368us/step - loss: 2.3389 - acc: 0.0000e+00 - val_loss: 0.1663 - val_acc: 0.0000e+00\n",
      "Epoch 479/500\n",
      "1074/1074 [==============================] - 0s 385us/step - loss: 2.3154 - acc: 0.0000e+00 - val_loss: 0.1504 - val_acc: 0.0000e+00\n",
      "Epoch 480/500\n",
      "1074/1074 [==============================] - 0s 413us/step - loss: 2.2871 - acc: 0.0000e+00 - val_loss: 0.1421 - val_acc: 0.0000e+00\n",
      "Epoch 481/500\n",
      "1074/1074 [==============================] - 0s 389us/step - loss: 2.2632 - acc: 0.0000e+00 - val_loss: 0.1489 - val_acc: 0.0000e+00\n",
      "Epoch 482/500\n",
      "1074/1074 [==============================] - 0s 371us/step - loss: 2.2364 - acc: 0.0000e+00 - val_loss: 0.1398 - val_acc: 0.0000e+00\n",
      "Epoch 483/500\n",
      "1074/1074 [==============================] - 0s 402us/step - loss: 2.2308 - acc: 0.0000e+00 - val_loss: 0.1590 - val_acc: 0.0000e+00\n",
      "Epoch 484/500\n",
      "1074/1074 [==============================] - 0s 445us/step - loss: 2.2223 - acc: 0.0000e+00 - val_loss: 0.1605 - val_acc: 0.0000e+00\n",
      "Epoch 485/500\n",
      "1074/1074 [==============================] - 0s 448us/step - loss: 2.2086 - acc: 0.0000e+00 - val_loss: 0.1619 - val_acc: 0.0000e+00\n",
      "Epoch 486/500\n",
      "1074/1074 [==============================] - 0s 451us/step - loss: 2.1986 - acc: 0.0000e+00 - val_loss: 0.1519 - val_acc: 0.0000e+00\n",
      "Epoch 487/500\n",
      "1074/1074 [==============================] - 0s 403us/step - loss: 2.1798 - acc: 0.0000e+00 - val_loss: 0.1389 - val_acc: 0.0000e+00\n",
      "Epoch 488/500\n",
      "1074/1074 [==============================] - 0s 384us/step - loss: 2.1587 - acc: 0.0000e+00 - val_loss: 0.1539 - val_acc: 0.0000e+00\n",
      "Epoch 489/500\n",
      "1074/1074 [==============================] - 0s 373us/step - loss: 2.1448 - acc: 0.0000e+00 - val_loss: 0.1335 - val_acc: 0.0000e+00\n",
      "Epoch 490/500\n",
      "1074/1074 [==============================] - 0s 377us/step - loss: 2.1327 - acc: 0.0000e+00 - val_loss: 0.1810 - val_acc: 0.0000e+00\n",
      "Epoch 491/500\n",
      "1074/1074 [==============================] - 0s 376us/step - loss: 2.1205 - acc: 0.0000e+00 - val_loss: 0.1564 - val_acc: 0.0000e+00\n",
      "Epoch 492/500\n",
      "1074/1074 [==============================] - 0s 372us/step - loss: 2.0800 - acc: 0.0000e+00 - val_loss: 0.1519 - val_acc: 0.0000e+00\n",
      "Epoch 493/500\n",
      "1074/1074 [==============================] - 0s 376us/step - loss: 2.0630 - acc: 0.0000e+00 - val_loss: 0.1633 - val_acc: 0.0000e+00\n",
      "Epoch 494/500\n",
      "1074/1074 [==============================] - 0s 377us/step - loss: 1.9923 - acc: 0.0000e+00 - val_loss: 0.1805 - val_acc: 0.0000e+00\n",
      "Epoch 495/500\n",
      "1074/1074 [==============================] - 0s 390us/step - loss: 1.9175 - acc: 0.0000e+00 - val_loss: 0.1223 - val_acc: 0.0000e+00\n",
      "Epoch 496/500\n",
      "1074/1074 [==============================] - 0s 378us/step - loss: 1.8387 - acc: 0.0000e+00 - val_loss: 0.1975 - val_acc: 0.0000e+00\n",
      "Epoch 497/500\n",
      "1074/1074 [==============================] - 0s 379us/step - loss: 1.8847 - acc: 0.0000e+00 - val_loss: 0.2021 - val_acc: 0.0000e+00\n",
      "Epoch 498/500\n",
      "1074/1074 [==============================] - 0s 366us/step - loss: 1.8170 - acc: 0.0000e+00 - val_loss: 0.1274 - val_acc: 0.0000e+00\n",
      "Epoch 499/500\n",
      "1074/1074 [==============================] - 0s 370us/step - loss: 1.7843 - acc: 0.0000e+00 - val_loss: 0.1640 - val_acc: 0.0000e+00\n",
      "Epoch 500/500\n",
      "1074/1074 [==============================] - 0s 378us/step - loss: 1.7863 - acc: 0.0000e+00 - val_loss: 0.1361 - val_acc: 0.0000e+00\n",
      "Train Score: 1.58 MSE (1.26 RMSE)\n",
      "Test Score: 0.15 MSE (0.38 RMSE)\n",
      "['loss', 'acc']\n",
      "valor: 8.068400 ---> Previsão: 8.012787 Diff: 0.055613 Racio: 0.006941\n",
      "valor: 8.214900 ---> Previsão: 8.012793 Diff: 0.202107 Racio: 0.025223\n",
      "valor: 8.268400 ---> Previsão: 8.012796 Diff: 0.255604 Racio: 0.031899\n",
      "valor: 8.216300 ---> Previsão: 8.012800 Diff: 0.203500 Racio: 0.025397\n",
      "valor: 8.240600 ---> Previsão: 8.012812 Diff: 0.227788 Racio: 0.028428\n",
      "valor: 8.357400 ---> Previsão: 8.012825 Diff: 0.344575 Racio: 0.043003\n",
      "valor: 8.285500 ---> Previsão: 8.012846 Diff: 0.272654 Racio: 0.034027\n",
      "valor: 8.221000 ---> Previsão: 8.012874 Diff: 0.208126 Racio: 0.025974\n",
      "valor: 8.173500 ---> Previsão: 8.012895 Diff: 0.160605 Racio: 0.020043\n",
      "valor: 8.195600 ---> Previsão: 8.012911 Diff: 0.182689 Racio: 0.022799\n",
      "valor: 8.099000 ---> Previsão: 8.012927 Diff: 0.086073 Racio: 0.010742\n",
      "valor: 8.054800 ---> Previsão: 8.012938 Diff: 0.041861 Racio: 0.005224\n",
      "valor: 7.884200 ---> Previsão: 8.012945 Diff: 0.128745 Racio: -0.016067\n",
      "valor: 7.821900 ---> Previsão: 8.012945 Diff: 0.191045 Racio: -0.023842\n",
      "valor: 7.811000 ---> Previsão: 8.012941 Diff: 0.201942 Racio: -0.025202\n",
      "valor: 8.020300 ---> Previsão: 8.012931 Diff: 0.007369 Racio: 0.000920\n",
      "valor: 8.119800 ---> Previsão: 8.012923 Diff: 0.106877 Racio: 0.013338\n",
      "valor: 8.055900 ---> Previsão: 8.012920 Diff: 0.042980 Racio: 0.005364\n",
      "valor: 7.802900 ---> Previsão: 8.012920 Diff: 0.210021 Racio: -0.026210\n",
      "valor: 7.717500 ---> Previsão: 8.012915 Diff: 0.295415 Racio: -0.036867\n",
      "valor: 7.532200 ---> Previsão: 8.012899 Diff: 0.480700 Racio: -0.059991\n",
      "valor: 7.751600 ---> Previsão: 8.012857 Diff: 0.261258 Racio: -0.032605\n",
      "valor: 7.799800 ---> Previsão: 8.012794 Diff: 0.212995 Racio: -0.026582\n",
      "valor: 7.861600 ---> Previsão: 8.012691 Diff: 0.151091 Racio: -0.018856\n",
      "valor: 7.759700 ---> Previsão: 8.012535 Diff: 0.252835 Racio: -0.031555\n",
      "valor: 7.848000 ---> Previsão: 8.012288 Diff: 0.164288 Racio: -0.020505\n",
      "valor: 7.850000 ---> Previsão: 8.011908 Diff: 0.161908 Racio: -0.020208\n",
      "valor: 7.790000 ---> Previsão: 8.011386 Diff: 0.221386 Racio: -0.027634\n",
      "valor: 7.802300 ---> Previsão: 8.010737 Diff: 0.208438 Racio: -0.026020\n",
      "valor: 7.857900 ---> Previsão: 8.009915 Diff: 0.152016 Racio: -0.018978\n",
      "valor: 7.894400 ---> Previsão: 8.009012 Diff: 0.114612 Racio: -0.014310\n",
      "valor: 7.758800 ---> Previsão: 8.008325 Diff: 0.249525 Racio: -0.031158\n",
      "valor: 7.643300 ---> Previsão: 8.008029 Diff: 0.364729 Racio: -0.045545\n",
      "valor: 7.644600 ---> Previsão: 8.007817 Diff: 0.363217 Racio: -0.045358\n",
      "valor: 7.782200 ---> Previsão: 8.007069 Diff: 0.224869 Racio: -0.028084\n",
      "valor: 7.761800 ---> Previsão: 8.005416 Diff: 0.243616 Racio: -0.030431\n",
      "valor: 7.914700 ---> Previsão: 8.002939 Diff: 0.088240 Racio: -0.011026\n",
      "valor: 7.951700 ---> Previsão: 7.999973 Diff: 0.048273 Racio: -0.006034\n",
      "valor: 8.094500 ---> Previsão: 7.999161 Diff: 0.095339 Racio: 0.011919\n",
      "valor: 8.079000 ---> Previsão: 8.001586 Diff: 0.077414 Racio: 0.009675\n",
      "valor: 8.153400 ---> Previsão: 8.004349 Diff: 0.149052 Racio: 0.018621\n",
      "valor: 8.178900 ---> Previsão: 8.006789 Diff: 0.172111 Racio: 0.021496\n",
      "valor: 8.156500 ---> Previsão: 8.008285 Diff: 0.148216 Racio: 0.018508\n",
      "valor: 8.098400 ---> Previsão: 8.009231 Diff: 0.089170 Racio: 0.011133\n",
      "valor: 8.125000 ---> Previsão: 8.010013 Diff: 0.114987 Racio: 0.014355\n",
      "valor: 8.152000 ---> Previsão: 8.010608 Diff: 0.141392 Racio: 0.017651\n",
      "valor: 8.122000 ---> Previsão: 8.011103 Diff: 0.110897 Racio: 0.013843\n",
      "valor: 8.096800 ---> Previsão: 8.011524 Diff: 0.085276 Racio: 0.010644\n",
      "valor: 8.078000 ---> Previsão: 8.011841 Diff: 0.066159 Racio: 0.008258\n",
      "valor: 8.099300 ---> Previsão: 8.012089 Diff: 0.087211 Racio: 0.010885\n",
      "valor: 8.045700 ---> Previsão: 8.012305 Diff: 0.033395 Racio: 0.004168\n",
      "valor: 8.028800 ---> Previsão: 8.012495 Diff: 0.016305 Racio: 0.002035\n",
      "valor: 7.924500 ---> Previsão: 8.012638 Diff: 0.088138 Racio: -0.011000\n",
      "valor: 8.080100 ---> Previsão: 8.012743 Diff: 0.067357 Racio: 0.008406\n",
      "valor: 8.077700 ---> Previsão: 8.012811 Diff: 0.064889 Racio: 0.008098\n",
      "valor: 8.130200 ---> Previsão: 8.012851 Diff: 0.117349 Racio: 0.014645\n",
      "valor: 8.252100 ---> Previsão: 8.012869 Diff: 0.239231 Racio: 0.029856\n",
      "valor: 8.271800 ---> Previsão: 8.012880 Diff: 0.258920 Racio: 0.032313\n",
      "valor: 8.260100 ---> Previsão: 8.012891 Diff: 0.247209 Racio: 0.030851\n",
      "valor: 8.298600 ---> Previsão: 8.012895 Diff: 0.285705 Racio: 0.035656\n",
      "valor: 8.295300 ---> Previsão: 8.012897 Diff: 0.282403 Racio: 0.035244\n",
      "valor: 8.309400 ---> Previsão: 8.012902 Diff: 0.296498 Racio: 0.037003\n",
      "valor: 8.274600 ---> Previsão: 8.012908 Diff: 0.261692 Racio: 0.032659\n",
      "valor: 8.290200 ---> Previsão: 8.012915 Diff: 0.277286 Racio: 0.034605\n",
      "valor: 8.243700 ---> Previsão: 8.012924 Diff: 0.230776 Racio: 0.028800\n",
      "valor: 8.281700 ---> Previsão: 8.012932 Diff: 0.268768 Racio: 0.033542\n",
      "valor: 8.444300 ---> Previsão: 8.012941 Diff: 0.431359 Racio: 0.053833\n",
      "valor: 8.495300 ---> Previsão: 8.012954 Diff: 0.482347 Racio: 0.060196\n",
      "valor: 8.584500 ---> Previsão: 8.012968 Diff: 0.571532 Racio: 0.071326\n",
      "valor: 8.569800 ---> Previsão: 8.012982 Diff: 0.556817 Racio: 0.069489\n",
      "valor: 8.450300 ---> Previsão: 8.012994 Diff: 0.437306 Racio: 0.054575\n",
      "valor: 8.238300 ---> Previsão: 8.013005 Diff: 0.225295 Racio: 0.028116\n",
      "valor: 8.201900 ---> Previsão: 8.013012 Diff: 0.188888 Racio: 0.023573\n",
      "valor: 8.152400 ---> Previsão: 8.013014 Diff: 0.139386 Racio: 0.017395\n",
      "valor: 8.182600 ---> Previsão: 8.013014 Diff: 0.169586 Racio: 0.021164\n",
      "valor: 8.201300 ---> Previsão: 8.013013 Diff: 0.188287 Racio: 0.023498\n",
      "valor: 8.216200 ---> Previsão: 8.013011 Diff: 0.203189 Racio: 0.025357\n",
      "valor: 8.292300 ---> Previsão: 8.013010 Diff: 0.279290 Racio: 0.034855\n",
      "valor: 8.298800 ---> Previsão: 8.013009 Diff: 0.285791 Racio: 0.035666\n",
      "valor: 8.300600 ---> Previsão: 8.013009 Diff: 0.287591 Racio: 0.035891\n",
      "valor: 8.348500 ---> Previsão: 8.013009 Diff: 0.335491 Racio: 0.041868\n",
      "valor: 8.389600 ---> Previsão: 8.013011 Diff: 0.376589 Racio: 0.046997\n",
      "valor: 8.400300 ---> Previsão: 8.013012 Diff: 0.387288 Racio: 0.048332\n",
      "valor: 8.373200 ---> Previsão: 8.013013 Diff: 0.360187 Racio: 0.044950\n",
      "valor: 8.421700 ---> Previsão: 8.013014 Diff: 0.408686 Racio: 0.051003\n",
      "valor: 8.465500 ---> Previsão: 8.013014 Diff: 0.452486 Racio: 0.056469\n",
      "valor: 8.492700 ---> Previsão: 8.013014 Diff: 0.479686 Racio: 0.059863\n",
      "valor: 8.513600 ---> Previsão: 8.013014 Diff: 0.500586 Racio: 0.062472\n",
      "valor: 8.510000 ---> Previsão: 8.013011 Diff: 0.496989 Racio: 0.062023\n",
      "valor: 8.478100 ---> Previsão: 8.013010 Diff: 0.465090 Racio: 0.058042\n",
      "valor: 8.496700 ---> Previsão: 8.013012 Diff: 0.483688 Racio: 0.060363\n",
      "valor: 8.449300 ---> Previsão: 8.013015 Diff: 0.436285 Racio: 0.054447\n",
      "valor: 8.567500 ---> Previsão: 8.013019 Diff: 0.554481 Racio: 0.069198\n",
      "valor: 8.498500 ---> Previsão: 8.013023 Diff: 0.485476 Racio: 0.060586\n",
      "valor: 8.490800 ---> Previsão: 8.013025 Diff: 0.477775 Racio: 0.059625\n",
      "valor: 8.472700 ---> Previsão: 8.013028 Diff: 0.459672 Racio: 0.057366\n",
      "valor: 8.511500 ---> Previsão: 8.013030 Diff: 0.498470 Racio: 0.062207\n",
      "valor: 8.536400 ---> Previsão: 8.013031 Diff: 0.523369 Racio: 0.065315\n",
      "valor: 8.578400 ---> Previsão: 8.013031 Diff: 0.565369 Racio: 0.070556\n",
      "valor: 8.614100 ---> Previsão: 8.013033 Diff: 0.601067 Racio: 0.075011\n",
      "valor: 8.645800 ---> Previsão: 8.013034 Diff: 0.632766 Racio: 0.078967\n",
      "valor: 8.659100 ---> Previsão: 8.013036 Diff: 0.646064 Racio: 0.080627\n",
      "valor: 8.683900 ---> Previsão: 8.013036 Diff: 0.670864 Racio: 0.083722\n",
      "valor: 8.700000 ---> Previsão: 8.013037 Diff: 0.686963 Racio: 0.085731\n",
      "valor: 8.723700 ---> Previsão: 8.013039 Diff: 0.710661 Racio: 0.088688\n",
      "valor: 8.679100 ---> Previsão: 8.013039 Diff: 0.666061 Racio: 0.083122\n",
      "valor: 8.501400 ---> Previsão: 8.013039 Diff: 0.488362 Racio: 0.060946\n",
      "valor: 8.498000 ---> Previsão: 8.013038 Diff: 0.484962 Racio: 0.060522\n",
      "valor: 8.396500 ---> Previsão: 8.013039 Diff: 0.383462 Racio: 0.047855\n",
      "valor: 8.351400 ---> Previsão: 8.013037 Diff: 0.338363 Racio: 0.042227\n",
      "valor: 8.385100 ---> Previsão: 8.013035 Diff: 0.372065 Racio: 0.046433\n",
      "valor: 8.406300 ---> Previsão: 8.013034 Diff: 0.393266 Racio: 0.049078\n",
      "valor: 8.498700 ---> Previsão: 8.013033 Diff: 0.485667 Racio: 0.060610\n",
      "valor: 8.494800 ---> Previsão: 8.013032 Diff: 0.481768 Racio: 0.060123\n",
      "valor: 8.478000 ---> Previsão: 8.013032 Diff: 0.464968 Racio: 0.058026\n",
      "valor: 8.567500 ---> Previsão: 8.013032 Diff: 0.554468 Racio: 0.069196\n",
      "valor: 8.525700 ---> Previsão: 8.013033 Diff: 0.512667 Racio: 0.063979\n",
      "valor: 8.489100 ---> Previsão: 8.013032 Diff: 0.476068 Racio: 0.059412\n",
      "valor: 8.451000 ---> Previsão: 8.013032 Diff: 0.437968 Racio: 0.054657\n",
      "valor: 8.421000 ---> Previsão: 8.013032 Diff: 0.407968 Racio: 0.050913\n",
      "valor: 8.417000 ---> Previsão: 8.013032 Diff: 0.403968 Racio: 0.050414\n",
      "valor: 8.398800 ---> Previsão: 8.013031 Diff: 0.385769 Racio: 0.048143\n",
      "valor: 8.414600 ---> Previsão: 8.013029 Diff: 0.401571 Racio: 0.050115\n",
      "valor: 8.401800 ---> Previsão: 8.013028 Diff: 0.388772 Racio: 0.048517\n",
      "valor: 8.551300 ---> Previsão: 8.013027 Diff: 0.538273 Racio: 0.067175\n",
      "valor: 8.539900 ---> Previsão: 8.013026 Diff: 0.526874 Racio: 0.065752\n",
      "valor: 8.565100 ---> Previsão: 8.013026 Diff: 0.552074 Racio: 0.068897\n",
      "valor: 8.600800 ---> Previsão: 8.013029 Diff: 0.587771 Racio: 0.073352\n",
      "valor: 8.589500 ---> Previsão: 8.013031 Diff: 0.576469 Racio: 0.071941\n",
      "valor: 8.789300 ---> Previsão: 8.013033 Diff: 0.776267 Racio: 0.096876\n",
      "valor: 8.888400 ---> Previsão: 8.013034 Diff: 0.875366 Racio: 0.109243\n",
      "valor: 8.891400 ---> Previsão: 8.013037 Diff: 0.878363 Racio: 0.109617\n",
      "valor: 8.914400 ---> Previsão: 8.013039 Diff: 0.901361 Racio: 0.112487\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xd4VFX+x/H3Sa8QCCF0Qi8BEjAIGkWKiAooioqrUgRWXV3FXXtZlV23/XRV1hUVQVQWESuirIggKCAgoZfQa6gppLfJ5Pv740wggSQESBv4vp7nPpOZuXPn3JOZz5x77rn3GhFBKaWU+/Co6QIopZQ6NxrcSinlZjS4lVLKzWhwK6WUm9HgVkopN6PBrZRSbkaDWyml3IwGt1JKuRkNbqWUcjNeVbHQBg0aSERERFUsWimlLkpr1qxJEpGwisxbJcEdERFBXFxcVSxaKaUuSsaY/RWdV7tKlFLKzWhwK6WUm9HgVkopN1MlfdylcTgcJCQkkJubW11vedHz8/OjWbNmeHt713RRlFLVqNqCOyEhgeDgYCIiIjDGVNfbXrREhOTkZBISEmjVqlVNF0cpVY2qraskNzeX0NBQDe1KYowhNDRUt2CUugRVax+3hnbl0vpU6tJUbV0lSil1MSoshO3bYcUKSEqCJ5+s+vfUUSWn2bdvHx9//PF5v/5vf/tbJZZGKVUbLVoELVqAvz94eUHnzjBuHEyaZIO8qmlwn0aDWylVnrlzYfBgCA6Ghx+G556D99+H+Hg4eBA8qiFVL5mukj/96U80aNCACRMmAPDcc88RHh7OI488UmK+p59+mvj4eKKjoxk9ejSPPPIITz/9NEuWLCEvL4+HHnqI+++/nyNHjjBixAjS09MpKCjg7bffZt68eeTk5BAdHU1kZCQzZ86siVVVSlWR//0Pbr0VLrsMvvsO6tevmXIYEan0hcbExMjp5yqJj4+nU6dOADz6KKxfX7nvGR0Nb7xR9vP79u3j1ltvZe3atRQWFtKuXTt+/fVXQkNDS8y3ZMkSXn31Vb799lsApkyZwvHjx3n++efJy8sjNjaWzz77jC+//JLc3Fyee+45nE4n2dnZBAcHExQURGZmZuWuXDmK16tSNSUxETZvhp49ISiopktTNRwO2yXi4wMrV9oWd2UyxqwRkZiKzHvJtLgjIiIIDQ1l3bp1HDt2jO7du58R2qVZsGABGzdu5PPPPwcgLS2NnTt30rNnT8aOHYvD4WDYsGFER0dX9SooVSscPgxLl8KGDXan3KZNsHOnfa5OHbj3XruDrkmTmi1nZfvgA9i1y3aVVHZon6saCe7yWsZVafz48XzwwQccPXqUsWPHVug1IsKbb77JoEGDznju559/Zt68eYwcOZInnniCUaNGVXaRlapRaWkwb57dGbd3L+zeDQcO2Oc8PaFtW4iMhPHjoX17+OwzmDwZfv4ZVq+281wMcnNh4kS44goYMqSmS3MJtbgBbrnlFl544QUcDkeZOyCDg4PJyMg4eX/QoEG8/fbb9O/fH29vb3bs2EHTpk1JSkqiadOm/Pa3vyUrK4u1a9cyatQovL29cTgcehi6m9i2zY4MaNbs4gmZC5WUBF9/DV9+CQsXQn4+hIZChw5w1VUQE2Nvo6Jst0Fxw4bB0KHwm9/A1Klw//01sw4XyumEJUtsP7a/Pxw6ZKf//hdqw+ETl1Rw+/j40K9fP0JCQvAs41varVs3vLy8iIqKYsyYMUyYMIF9+/bRo0cPRISwsDDmzJnDkiVLeOWVV/D29iYoKIiPPvoIgPvuu49u3brRo0cP3TlZizmd8MQT8Prr9r6PD/z+9/Dqq7Xji3mhHA4YO9au1//9nw3es83/008wZQrMmWPvR0TYOhk+HHr3rvhoiREj4J137GiL22+vvB14GRkwYQIcPw4jR8LNN4Of34UtUwQ+/hiSk+2Pd3Ky3VpYsMC+j6+vrYvCQrj+eujbt1JW5cKJSKVPl112mZxu69atZzxW3ZxOp0RFRcmOHTtquiiVpjbUq7tJSxO56SYREPnd70SmTBEZMcLe/9vfarp0ZzpyROTLL0WefVbk9ttF7rpL5Le/FXnvPftcXp7Ixo0iy5eLOJ0ihYUio0fb9fH0FAkPF3n3XZFFi+x82dl2uUePivzrXyL9+on4+9v569UTefRRkTVr7HLO14YNIh4eIrfeKvLBByLffCPicFTstXv2iMyebV/z8892HQ8cEImKsuvTtKkta0iIyP33i6xYcf5lnTrVLqv4FB5uPw+ffSaSlWXLnZAgkpt7fu9RUUCcVDBjL5ng3rJli7Rq1Ur++Mc/1mg5KltN16s7OXFCZOJEG04eHiJvvnnqOafTBiKI/Pe/NVfGIrt2iTz2mEjnzqcCxdNTpF07kTZtRBo0OPW4l9epvzt2FLnzTvv3xIki69aJdO9eMpg8PERat7bLAxuIjzwi8vnnIjk5lbcOTz1V8n3vuEMkP7/81yxYIBIcfGaYenjYx+fPt/+rhQtF7rnn1A9O+/Yif/6zyLZtFQ/xnTtFAgNF+vcXOXZMZO1akR07LuwH60KcS3DXyHDA2mDTpk2MHDmyxGO+vr6sWrWqhkp0fmpbvdZWeXm2b3bzZruJ/ac/2bG4p88zaBD8+ivs2QONGlVtmTIy4MgRCA+HunVtRP38s+2+mTvX9rn37w8DBkCfPrZP2d/fvlbErss339jldOtmN+lff90OtR03Dt57z3b7OJ324JCkJDh2zP69dSu0aQOjRkFVfnyOHoXsbPj0U3jmGVv3s2fbLojijh+Hzz+3XSGdOtn+cQ8P23Wxfbs9sGX0aOjSpeTr0tPtDtEZM2xXD0CrVnDllXbkR3Cw7Zvv3BkyM2HHDsjJsUc9vvaaXfbGjdC8edXVQUWdy3BAt21xFxZWbuvAXWmLu2JefNG2zObMKX++HTts6+7xx6umHGvW2O6Z0NCSrcmePUWio+39+vVtt8ihQ+e+/MJCka1bbau0tnnzTbt+rVrZLpr580UeeECkZctTddG/v0hq6vktf/9+kcmTRYYOFYmIEGnYUMTX98zWe/Fp1qxKXcULwsXe4haxLaITJyAs7NIeEXApt7hTUyElxbawytuhuGULdO8Od9xhRwWczciRdkTF3r3QsOGFl9PptC3oV16xJyLy87NH30VF2Vb97t12uF1Ojh2Fcc89EBBw4e9bG82bB//8px0HDnY9Bw2C2Fi7RRQba8/9UVkKC+3/ccuWU63vwEA7pNHDww5lrC3OpcVdq4O7sNCOn/T3P/XFFLGbTceP28H+6el2sysiouYHxdeEizG4MzLs5uuxY/YgjoAAG6SzZtlNZz8/G3IpKXb+iAgbhF5edlO4fn07GiI6Gtatg/vug337bBdBWNjZ33/7dru5/uST8I9/VLzcIvDRR/Dvf9vPZXa2nbKybDdGq1b2qOFRoyAk5Hxq5uKxfr09kKdfv1PdP5c6tz9ysrDQfkGPHLFjSP39oXFj+1xamn0uPNz2S2Vk2C/l9u12yFOzZqBDqN1Tbi7cdpttlZ3OGNvXO3Cgnc/HB1q3tq2nb76xYenhYfttDx60J/1p0sSGg4+PHfJVkdAG2yq78074z3+gVy+48cYz+2RPl5xsTzg0a5Zt3ffsaX9wiqaYGDvGuTJbk+4sOtpO6vzUuo+RiN10TEuzX8rwcHsehD177PMeHnbztVkzez842O54OHrUTikp0KCB3dmTk2ODPzz8wsd7XqxE7A/f3r12B1eDBjVXjrFjbWg/+SRcffWp4E1JsTvpiv7np/vd7+z/2sfHdpmlptqdW0uX2tC94w6oV+/cyvPSS/YAjFtvtZ+lUaPg8cdtY2H7dli82O7gi4+30+HD9r1ffhmefvrS7bpT1aSineHnMl3IzsmjR0VWr7ZjN4uG5RQW2h0WmZnl73TJyRHZt08kLs4uY/Vq+/e6dfa1lSkwMFBERA4dOiTDhw8vd97XX39dsrKyTt6/4YYb5MSJE5VSjvPdOZmba3eANWtWcmdNmzYir712asyqwyGycqUd33z77XaI2fLlZx/WdS4KC0Wef15q3Thqh8PuQLv7bjvkzsur5I60oCC7U3HUKJG//90OJ1PqfFHZOyeNMX8AxgMCbALuFZEyL3Z4vn3c2dm29VKnjj0HwvkeweZwnOobLyiw/Z4FBdCypW15lXUEmNPpLHFEZWambf0HB9uWVvEumHM5C2BERARxcXE0qILmbEX7uDMybHdB06a2fidMsP2MN90E111n63vDBnuI75IlthuiUyc7PK3oDAAtWthuCBF7BrhrrrFTVJTtZ9661fZN9+5tuzTK+v8VFNjDh3187JC2iRNh+fKSQ9hqmwMH7FC7/fvtug0adPadokqdi0odDgg0BfYC/q77nwJjynvN+bS4nU6RTZtE1q+v3NaciD2y7Pvv90rLlh1kyJBR0qlTVxk+fLhkZWVJy5YtZeLEiRIbGyuzZs2SXbt2yaBBgyQ6uodER18lX30VL3FxIt98s0diYnpLTEyMPP/88ydb3Hv37pXIyEgRESkoKJDHHntMunTpIl27dpV///vfMmnSJPH29pYuXbpI3759RUSkZcuWkpiYKCIi//rXvyQyMlIiIyPl9ddfP7nMjh07yvjx46Vz584ycOBAyS463O00FWlxFxba1nLxlnVYmMjcuaXPP3++SEyMPajh/vvtUWzHjtnnkpLsgRoPPGAPBilrmFVsrH1dfHzJ/2d+vkivXiXnbdZM5K23Kn5knVIXI86hxV3RPm4vwN8Y4wACgMPn+GNSUikn5DZAp6xcjDnPK0h06gQvvFDqUz5AO/9D7N+/nXdeepnLukzkiX88yeSXX4aCAvyys1nmGic24O67eXPiy4SFtGLd1vX8/Z9j+eG/s3j+j/cxZsjtPDh6OJNnfGQzZ/9+SEiwTfz9+5kyYwZ7t2xh3ddf4+XlRUpqKvVDQnjtlVdYPGMGDerXt003pxMSElizahXTp0xh1Zw5iAi9br6Zazp0oF7duuzcuZNZr7/Oey+8wB2/+x1fTJnCPbfeWqzCXE291FTbIevlZTtWi09eXuDtzc/LvfD7zJsvhnvSrbOTlGMOOrYtoM5+B7xRYMtfUGAnh4NBBQUM6lvsscUFsBgQIVSE4cBwEegLub2ElBTIzBDqhkC9usLu3bBhrZA5AlYgrPGBAf2ERuHClg3w4Abh3W7g7y94e0OLZoLnL8By19afnHarlLuoW9eeqKWKnTW4ReSQMeZV4ACQAywQkQWnz2eMuQ+4D6BFixbnXBADeOK0nTHOc365DZ9iZ/U7Y/mZGTRv1IhB3dvjcKRz17UDmTz7YygsZMQ110BaGpnZ2fyyZg2/eegBEJuNeQ4HXpmp/LphNV/88+/IiROM7NePp/7+dzuQPD3dBvGJEyxcvJgHbr8dr/R0AOqD3bPmdNrboiBy3V+2ZAm39OlDYE4OALdecw1LFy/mpj59aNWkCdENG0JiIpe1bs2+7dvtXtoiRctKS7N9DeW4xjXxhWs6G2/vk6F/8geh6Iei+K0x+AFNTnuukzF0rA/5BYYCB6SlG/IXQHo9Q90Uww2BEJZhIMP1msOUvvzit0q5g2rau3/W4DbG1ANuBloBqcBnxph7RKTEoQwiMgWYAraPu9yF1sQJuevUwfj4YKK6kZ0Ku1cnkecXAj4+BPbsCQ0aUJieTt2QekybuZ3mze1olJM8PdnhF42fnxdN2qTbzYLoaDsg188PoqORunUxbdueOc7Jx6fkkA3Xfdm40XbEd+9uH2/UyI5Z69IF3zp1oEcPRMBj0Y8UZGVCjx5nrld8vB0/6XTa1rHTCU4nn85yMu3dAjatc9C4QQHff+ugQT2nDeLioXz6rYdHpYSlAXxdU+YxGNjf9oE3bmz7tamhSz4pdTGoSKfEtcBeEUkUEQfwJXBl1Rarahw4cIAVK1YQEgJLlsyiY8erKCw81XitU6cOjRu3YvHizwgLs/3/GzZsACA2Npblyz8hMxOmTy/9dK3XXXcd77zzDgUFBQCkpKSQnQ1BQSXP8Q02X2Nj+zBnzhyys7PJysriq6++4uqrrwZsFh84YHcYJiWdpdfAGBu6fn4QGMj3K+ow4oF6HMoP4w//14RvN7agQa829kz3rVvbvYyNG9sfiZAQu6fR17dky7oShYfDjz/aU3x+/HHNXadPqYtFRYL7ANDbGBNgjDHAACC+aotVNTp16sSHH35It27dyM9P4e67f0dBgR05kpRkj8Z86aWZfPfdNLp3jyIyMpKvv/4agEmTJjFjxluMHt2Tw4fTSl3++PHjadGiBd26daNbtygmTfqYrVth6ND7uP76G+jXrx9Hj9qx5Zs2gTE9uPHGMcTEXE6vXr0YP3487dt3Z88ee8KjxETbIM/NLbcXqIQTJ+x46M6dIS7OnnO66OClmhQebk80VGvOZ6yUG6vocMCJwAigAFgHjBeRvLLmr41nB9y3bx9Dhgxh8+bNJx8TsWF9+LBtAYPtMejatewdpPv22e7qNm3sfojT5eXZQ7WLWslhYfaoOk9P27BNSTnVyHU6bTgXFNiALtpH6OV1qtfE09O+Z1KSHbJ3+qHS8fHxZGV1Yu5c2xvz2Wf28PCVK888+51Sqvaq9EPeReRF4MULKlUtZIxtCYaF2Vaww2G7n8sb1dKkiT33xM6d9ki+8PBTvQtJSTZkjbHdAU2a2B6I0FA7ljwlxbZ+mzQ59ZrwcHvEZ06ODXM/Pzt/8UOjW7SwY9z37rWHYxc/AZHTaa+Bd+zYqccmTtTQVupiVusOea8qERERJVrbxXl42MCsyGHxPj7QsaMN6IQE2yIOC7N90ocO2cP027QpeS2+wEA7WjE/3x78UpyXV9mHchcvX5s29vqIO3fa9/f1tS36pCQ7sKVoA2f/fntQjVLq4nXJBHdl8vS0+/i2b7dhXa+ebU0XnQHu9AuoQsV/GMri6wvt2tn33LnTdpnk5trp7bdPtbC1pa3Uxe98DnVR2K6O5s1tn/SRI7a7IzCwak8tGxBg+7kLCmzXSFqabcGPHl1176mUqn20xX0BAgNtf3RR/3KLFlV/vEhwcMlh4vFuOb5HKXUhtMV9gZo2tX3Q/v6ljzJRSqnKdskG90svvcSrr77KCy+8wMKFCwFYunQpkZGRREdHk5OTwxNPPEFkZCRPPPFEmcvx8bHHtbRpo0dnK6WqxyXfVfLnP//55N8zZ87k8ccf59577wXg3XffJTExEd+zXP4kKKhKi6iUUiVcUsH917/+lY8++ojmzZsTFhbGZZddxpgxYxgyZAipqal8+umnfP/99yxcuJCMjAyysrLo1asXzzzzDCNGjKjp4iulFFBDwf3o/EdZf3T92Wc8B9GNonnj+rJPXrVmzRo++eQT1q1bR0FBAT169OCyYmPnxo8fz7JlyxgyZAi33XYbYC+WsH595ZZTKaUu1CXT4l66dCm33HILAa7DDm/So1SUUm6qRoK7vJZxVTK691ApdRG4ZEaV9OnTh6+++oqcnBwyMjL45ptvarpISil1Xi6ZrpIePXowYsQIoqOjadmy5cnzXiullLup0Gldz1VtPK3rxUrrVamLw7mc1vWS6SpRSqmLhQa3Ukq5GQ1upZRyMxrcSinlZjS4lVLKzWhwK6WUm9HgroC4uDgeeeSRmi6GUkoBl9ABOMWJCCKCR3mXcy8mJiaGmJgKDa9USqkqd8m0uPft20enTp148MEH6dGjB+PGjSMmJobIyEhefPHFk/OtXr2aK6+8kqioKC6//HIyMjJYsmQJQ4YMASAlJYVhw4bRrVs3evfuzcaNG2tqlZRSl6gaanE/ClT26VKjgfJPXrV9+3amT5/O5MmTSUlJoX79+jidTgYMGMDGjRvp2LEjI0aMYPbs2fTs2ZP09HT8/f1LLOPFF1+ke/fuzJkzhx9//JFRo0bpqV+VUtXqkuoqadmyJb179wbg008/ZcqUKRQUFHDkyBG2bt2KMYbGjRvTs2dPAOrUqXPGMpYtW8YXX3wBQP/+/UlOTiYtLY26esFJpVQ1qaHgrpnTugYGBgKwd+9eXn31VVavXk29evUYM2YMubm5iMhZT/1a2rld9HSxSqnqdMn0cReXnp5OYGAgdevW5dixY3z33XcAdOzYkcOHD7N69WoAMjIyKCgoKPHaPn36MHPmTACWLFlCgwYNSm2ZK6VUVbmkukqKREVF0b17dyIjI2ndujWxsbEA+Pj4MHv2bB5++GFycnLw9/c/eQX4Ii+99BL33nsv3bp1IyAggA8//LAmVkEpdQnT07q6Oa1XpS4OelpXpZS6iGlwK6WUm6nW4K6KbplLmdanUpemagtuPz8/kpOTNWwqiYiQnJyMn59fTRdFKVXNqm1USbNmzUhISCAxMbG63vKi5+fnR7NmzWq6GEqpalZtwe3t7U2rVq2q6+2UUuqiVaGuEmNMiDHmc2PMNmNMvDHmiqoumFJKqdJVtMU9CZgvIrcZY3yAgCosk1JKqXKcNbiNMXWAPsAYABHJB/KrtlhKKaXKUpGuktZAIjDdGLPOGDPVGBN4+kzGmPuMMXHGmDjdAamUUlWnIsHtBfQA3haR7kAW8PTpM4nIFBGJEZGYsLCwSi6mUkqpIhUJ7gQgQURWue5/jg1ypZRSNeCswS0iR4GDxpgOrocGAFurtFRKKaXKVNFRJQ8DM10jSvYA91ZdkZRSSpWnQsEtIusBvcy5UkrVAnp2QKWUcjMa3Eop5WY0uJVSys1ocCullJvR4FZKKTejwa2UUm5Gg1sppdyMBrdSSrkZDW6llHIzGtxKKeVmNLiVUsrNaHArpZSb0eBWSik3o8GtlFJuRoNbKaXcjAa3Ukq5GQ1upZRyMxrcSinlZjS4lVLKzWhwK6WUm9HgVkopN6PBrZRSbkaDWyml3IwGt1JKuRkNbqWUcjMa3Eop5WY0uJVSys1ocCullJvR4FZKKTejwa2UUm5Gg1sppdyMBrdSSrkZDW6llHIzGtxKKeVmNLiVUsrNVDi4jTGexph1xphvq7JASimlyncuLe4JQHxVFUQppVTFVCi4jTHNgMHA1KotjlJKqbOpaIv7DeBJoLCsGYwx9xlj4owxcYmJiZVSOKWUUmc6a3AbY4YAx0VkTXnzicgUEYkRkZiwsLBKK6BSSqmSKtLijgVuMsbsAz4B+htj/lulpVJKKVWmswa3iDwjIs1EJAK4E/hRRO6p8pIppZQqlY7jVkopN+N1LjOLyBJgSZWURCmlVIVoi1sppdyMBrdSSrkZDW6llHIzGtxKKeVmNLiVUsrNaHArpZSb0eBWSik3o8GtlFJuRoNbKaXcjAa3Ukq5GQ1upZRyMxrcSinlZjS4lVLKzWhwK6WUm9HgVkopN6PBrZRSbkaDWyml3IwGt1JKuRkNbqWUcjMa3Eop5WY0uJVSys1ocCullJvR4FZKKTejwa2UUm5Gg1sppdyMBrdSSrkZDW6llHIzGtxKKeVmNLiVUsrNaHArpZSb0eBWSik3o8GtlFJuRoNbKaXcjAa3Ukq5GQ1upZRyM2cNbmNMc2PMYmNMvDFmizFmQnUUTCmlVOm8KjBPAfCYiKw1xgQDa4wxP4jI1ioum1JKqVKctcUtIkdEZK3r7wwgHmha1QVTSilVunPq4zbGRADdgVVVURillFJnV+HgNsYEAV8Aj4pIeinP32eMiTPGxCUmJlZmGZVSShVToeA2xnhjQ3umiHxZ2jwiMkVEYkQkJiwsrDLLqJRSqpiKjCoxwDQgXkReq/oiKaWUKk9FWtyxwEigvzFmvWu6sYrLpZRSqgxnHQ4oIssAUw1lUUopVQF65KRSSrkZDW6llHIzGtxKKeVmNLiVUsrNaHArpZSb0eBWSik3o8GtlFJuRoNbKaXcjAa3Ukq5GQ1upZRyMxrcSinlZjS4lVLKzWhwK6WUm9HgVkopN6PBrZRSbkaDWyml3IwGt1JKuRkNbqWUqiRpuWnV8j5nvXSZUkqpkvKd+czYMIPE7ERyC3LZmriVFQkr8PLwYu+EvVX+/hrcSil1DrId2dz26W18t+u7k4+1rNuSq1pcRe+mvSmUQjxM1XZmaHArpVQFZeRlMGTWEJbuX8q7Q95lZLeR+Hr5VnlQn06DWymlKugP3/+B5QeWM/PWmfym629qrBwa3Eqp85JbkMvO5J1sPLaRXw7+wubEzQxoNYD7L7uf8KDwmi5epduetJ3p66fz8OUP12hoAxgRqfSFxsTESFxcXKUvVylV/USEhPQE9pzYw+4Tu1lxcAXLDi5je9J2BJsfQT5BtK3flvVH1+Pt4c0rA19hQu8JNVzyyjXi8xHM2zGPPRP20DCwYaUv3xizRkRiKjKvtriVUmcQEeIOx/FF/Bd8Gf8lO1N2nnwuxC+E2OaxjIgcQacGnegc1pnOYZ3x9PBkR/IOHvnuEZ5c+CSD2w+mbf22NbgWFy4tNw1/b3+2Jm7l0y2f8vzVz1dJaJ8rbXGrS9bulN08++Oz+Hv507Z+W0ZEjqBdaLuaLlal2XhsI/5e/ue0TlsTtzJ17VQ+3/o5B9MP4uXhRb+IfgxtP5SODToSERJBm/ptyt0ZdyTjCO3/056+EX355jffVMaqnLQzeSepuanENInBGFMpyxSRk8sSEXaf2M38XfOZvWU2yw4sA8DPyw9/L3/2TNhDiF9Ipbzv6c6lxa3BrS5Jyw4sY9gnw3AUOgj2CeZQxiEaBjZk5biVtKrXqqaLV6psRzZ7T+zFx9OHAO8AmgQ3KTO8VhxcQf+P+uMsdPLiNS/yZOyTeHt6nzFfvjOflQkrWbp/Kd/t+o7lB5fj7eHNDe1uYHin4QxpP4T6/vXPuayvLH+FJxc+yby75nFjuxvP+fWlmbFhBuPmjsNR6KBDaAfGRI9hZLeRNK3T9LyXmZCeQN8P+pKck0yzOs1Izk7mSOYRALo07MKtHW/Fw3iQkJ7A4PaDGdZxWKWsS2k0uKvZX376C63rtebubnfXdFHUWaTmpvLmqjd5eenLRIREMO+uebSt35b4xHhi34+lYWBDfhn3y3mFVWVLzk5m0d5FLNqziBUJK9iauBWnOE8+3zS4KUPbDyU8KJyNxzaSkZ/B2OixRDWKos+LyzYuAAAWKklEQVT0PtTzr0f3Rt35bOtnNK/TnJYhLQkLCKNDaAc6NujI6sOrmbV5Fik5KQB0bdiVUVGjGB01mrDAsAsqe74zn25vd+NA2gHCg8IJ9Q/lzRve5IrmV5z1dY99/xgL9y4kwDuAur51aR/aHhFhytop9G/Vnzsj72TGxhksPbAUD+PBwNYDGRU1imEdhxHgHVDhMhZKIQNnDGRVwipGR43mUMYhgnyCuLrF1fSN6EuHBh0uqA7OlQZ3NVq6fyl9PuhDPb967H90P8G+wTVdJFWGd+Le4amFT5Gel84tHW9h6k1TSwT00v1LGThjIFc0v4IfR/1YaZvi52r90fW8vvJ1Zm2adXKLILZFLDGNY+gc1plCKSQ1N5XF+xYzf9d8cgpyaFe/HQWFBew+sRuDoUFAA1aMW0Gb+m2Ys20OMzfNJCk7iWOZx9iZspOCwgJ8PX0Z1nEYd3a5k6tbXE1oQGilrseW41t4a/VbZDmy+Hn/zyRmJTLvrnlcE3FNqfOn56Vz26e38cOeH7ix3Y14GA+Ss5PZlrSNE7knGNd9HJMHT8bH0weAXSm7+GjDR3y44UMOpB0gyCeIwe0Gc0PbG7iy+ZUE+wYT7BNMoE9gqe/3+orX+eOCP/Le0PcY32N8pa77+dDgriaFUkivqb3YlbKL1NxUXh34Ko9d+ViVv++BtAOk5qbi5eFFh9AOeHp4Vvl7urs1h9dw+dTL6RvRl39d9y+iG0WXOt+7ce/ywLwHmHvnXIZ2GFolZTmUfogv479kb+pejmQeoWlwU/q36o/D6eCNVW+wZN8SAr0DuTf6Xu7udjcxTWLw8ih9HEG+Mx9noRN/b38KpZB5O+Yxc9NMHr/ycWKalJ4B+c58dqXsomlwU+r61a2SdTzdkYwjDPhoAPtS9/HvG/7NPd3uwc/LjyMZR5i/az6rD6/m+93fcyDtAO8NfY8x0WNOvlZEyHZklxnAhVLI0v1L+e/G//Ltzm85mnm0xPNNg5sS2TCSzPxMtidtJ6cgh5Z1W7L7xG6ub3s9c0bMqbEf6eIuieD+ad9PvLf2PV7q+1KN7bmesWEGo+aMYsYtM5i+fjrxifHsmbAHPy+/KnvP99e9z7i5407ev7b1tcy/e76GdzkcTgc93+vJ8azjbH1oa7k7lxxOBx3+04HQgFB+Hf9rpe4AW7B7Af9Z/R/+t/N/FEohAd4BNApqxKH0Q+Q58wBoUbcFD1/+MON7jK+ynWA15XjWcW6adROrDq0iLCCMNvXbsCphFYJQx7cOlzW+jGeueoaBbQae93sUSiEbjm5g47GNZDuySc1NJT4pni2JWwj2CaZDaAcCvAM4kH4AD+PBWze+VStGicAlENzrj66nz/Q+ZORnEOAdwKsDX+WBmAeq9VfzaOZRYqbE0Di4MavGr2LJviUM+GgA7wx+h/tj7q+S95y7fS63zL6Fa1tfy3097mPT8U1M/Gkif+rzJ/7c789V8p61RUZeBnO3zyU5Jxk/Lz9yHDnsObGH5JxkBrQawLCOw/D29GZn8k5C/EJK7GD8x7J/8MyiZ/jyji+5pdMtZ32vaWunMf6b8fzvrv9xQ7sbzqmc2Y5sFu1ZRHpeOtmObLIcWWTkZfB5/OdsPLaRRkGNGBs9lnu730ubem0wxpDjyOGXg7+QU5DD9W2vL7N1fTEQERbvW8ybv77J4YzDDG0/lJs73Exkw8hqP2y8trkogjuvII+pa6eyYM8C7ul6D8M7D8fDeLArZRdXT78abw9vPrv9M15Y8gILdi+gT8s+vDP4HTqFdaqktShb3OE4hn0yjBO5J/hx1I/0atYLEeGKaVewN3UvL/d7mZFRI0u0vD9c/yGbjm/i5f4vl9oiP9uJaX45+AsDPhpAt/BuLBq1iCCfIADGfj2WD9Z/wPSbp+Pl4cWhjEM81POhMjcra7tlB5YxZ9sc1h9dz7GsYzQJboK/lz8Ldi8gpyCnxLxBPkEEegdyLOsYHsaDQikEwGAY0n4Ig9sN5psd3zB/13xu7ngzX9zxRYXKkO/Mp/2b7WkU1IgV41ZUuEGw4egG7vziTrYlbTvjuciwSB6/8nHu6nrXyT5apYpz++D+bud33P/t/RxMP0h9//qk5KTQsUFHRITtydup51ePZWOX0TmsMyLCtHXTePKHJ8nMz+TBng/yh95/oGVISxxOB0nZSTQOblzme4kIC/csZNPxTRxKP0R0o2hGRo0sc/6FexYydNZQwgPD+frOr4lqFHXyufVH1zNu7jjWHllLo6BG/KXfXxjbfSyTVk7ijwv+CMCVza/kqxFfndw8+2H3Dzz+w+PEJ8bTvG5zOod15qnYp7iqxVUnl3s08yg93u1BoE8gK8atoEFAg5PPZTuy6TW1F5uPbz752IMxD/LW4LfOveJr2JJ9S7huxnV4enjStWFXmgQ34XDGYZJzkk+OHOjYoCM5jhx8PH1O1kPc4Ti+2fENfl5+tKvfjs3HNzM5bjJJ2Uk0DW7K3V3v5pmrnzmnrocpa6Zw/7f307VhV+7qehejo0af8TlKyk4iPjGe+KR4Nh7byNS1dmfn24PfpnNYZwK8AwjwDsDf2x9fT99a0Y+qai+3Du7dKbuJfjeaiJAIXh/0Ov0i+vHplk9589c3CQ0IJbZ5LLd1vu2Mfu3ErESeXvg0H238CBEhqlEU8Ynx5BTk8PDlD/PG9W+c0aLdmriVR757hEV7FwHgYTzwMB5se2gbbeq3OaNsmfmZRE6OJNA7kJ/G/FTqkKmiTcEXFr/A8oPLaVu/LbtSdjG803CGdxrO2LljCfELoX1oe3IcOaw+vJpWIa0Y3mk4CRkJLNm3hKOZRxnafih/6P0HYlvEMnDGQFYfWs2q8avoGt71jPc8lH6I5QeXExkWyXtr32PSqkksHLmQAa0HlFvXvxz8hdmbZ7Py0EoOph3ksiaXcWWzK7m3+700CmpUYt6MvAw2H99M57DOVbJDa1vSNq6YdgWNghrxy9hfqOdf74KWl+PIYUfyDro07HJe/f+FUsi7ce8yY+MMViSswMfThzFRY+jdrDff7/6exfsWczzr+Mn5/b38Gdx+MJNvnHzBQ+nUpanSg9sYcz0wCfAEporIP8qb/3yD2+F0cPX0q9mevJ0ND2ygRd0W57yMg2kHmbRqEmuOrCE6PJr0vHTeX/8+d3e9m+k3T8fb0xsRYfLqyTz6/aME+QTxl35/4e6ud5NbkEvbN9tyU4ebmDV8Fs5CJ9PWTeOqFlfROawzj33/GK+tfI1l9y4jtkVsueUQET7e9DFPL3qa/q36M+2maXh5eLH60Gr+/POfycjLwClObmp/E4/0egRfL1/AtqAnrZzEP5f/k7S8NIJ9gsnIz2DGLTO4p9s9Z13/HEcO0e9Gk1uQy6bfbaKObx3Abv5vOLqBiJAIgnyCeHbRs7yx6g38vfzp2bQnzes0Z82RNWxL2kagdyDPXPUMPRr34Kf9P7Fk3xLiDsfhFCeexpPLm17Ota2vZWDrgfRq1qvEpn9eQR4+nj6lti7zCvLw9vQu8QMqIvyw5wfu//Z+svKzWDV+Va07AGZXyi7+9cu/mL5+OnnOPBoFNeK6NtcRFR5Fpwad6BTWiRZ1W1yifbROIBfIAfKBACAYGxUXGwH2AOuAA8AhIA8oKDYFA2+e19IrNbiNMZ7ADmAgkACsBn4jIlvLes35BveffvwTLy99mdm3zeaOyDvOMncBkAKEAGX3GYoI/1j2D5798Vk6hHbg95f/ni3Ht/DOmncY0n4I026aVmKv8vM/Ps9fl/6VFeNW8NqK1/hs62d4eXhxb/S9TFs3jfHdx/Pu0HcrvE7FD6c9F9mObOZun8snmz8hulE0L/V9qcKvXZmwktj3Y2lWpxmjo0bj5+XHf379z8kjwnw9fclz5vFQz4f457X/LNEfvjN5J08tfIqvtn0FgLeHN5c3tcPoejTuwfqj6/lhzw/8euhXCqUQH08fIsMiaVWvFfGJ8WxL2kaPxj34S7+/cH3b6zHGUCiFTFo5iWd/fBZvD2+6N+5OREgEPh4+bE7czMqElTSv05zP7/icy5tefs51VTHJ2C/cISADyHTdZhS7XxdoC0QCVwElW/3HMo9xPOv4ee5Ic2LDLZdTQVf0twfgBxhXOU6fMlzzFbqW43T97Qv4Y0MyHxsswUAg9qu6yzVfmGvdcN0X11T0d0GxspQ25ZTzXEEZ69sSuBzo6CqfF9DE9XgYUAfwPu09coBjwEHXOhe9ztNVN8ex/7981zoWTUGn3Q901Ytx1W1ZtznAFmArkFWsfgtd65XlmgfX/DuBI8XWMcD1Pl7FpnBgVRl1Ur7KDu4rgJdEZJDr/jMAIvL3sl5zPsGdkpPIuK8juKNLJL/pEgnsB1KBbOyHsS62QvOw/9z9nPrQBAGhQH3XbajrsXzsPyKcjcdS+WLrEtYf28uRDLiuzR+Z2Pf/ztiMTstNo+N/WhNRL5NGQfnc12MwRzNT+Xr7chKzQvn2rh3UqwVH1Z3N/F3zeWPlGyzY/T2NgmB0VA9u6RRFWm4mRzOTuLxpRzo0CAHSONVq8Mb+CPpwJCOZ3AInTYNb4ePl55onz/W8LzkOYUfyAXalHGTPicMczkyiRZ1GNK8bztIDa9ielESQT3MaBXVmd8oJFuz5lRvaDiYyLJxsxwqEJJyFBQT6+HFzh75c0aw9Xp4HsF+OVE4FVEGxv0+/78B+JvyxXyJcj3lhg8Ef+/lJA0qO7bU8sZ+torA7gf1sgf1yd8V+EYOLTUVhU1oIl/dYWQF3PoqCx3mW+Zph6yIRG0LFFQWYwdaDP/bHo+i2tOlsz3lj1zcV2Ab8ClTmZby8seHv61qfoslxgcttgG0AehSbPDn1AwD2/9ccuBr7g9TK9ZrK229R2cF9G3C9iIx33R8J9BKR35f1mvNrcRdSKMF4mGygEdAaG8QB2F/fVGxl+mKDua1rvjRsa6poSnHdZmFDxgP7pc0+7f08gU6uZYRiP3xewH4czp/w9izrw9AWuA7bcmgKdMe2KnKApcAmVxlSsUGQiv3Ct3StyyFsy6HoSxfsWk9/7Icjz7W+aUC6awoC2heb2rjKK9iW1RbXOvq4lrsV2AgcRyQLY8r6H3thfxB9XX8XYH/siiZHsXIa13z52LA8NyIGCMKYjHLmagS0w36RPCnZ4irtvjenWk7ZrjJ6u8qd7no8AFt/HbH/q1acCuGiVm5x6cB6YDG25XSCki1zB6WH3Lk+VvS3L/b/WNSiDnKVLei0yc+1zkWBC6da8U7XcuDU1kMjToUOrmUbKjNoKqaodZ+P/ezvx34/MlyPnV5HDbE/OHUp+YNdiK2X0rZ0HJQM8kxsfZ6+ZXH6rRfQGfvjXPMq+7Supf2nz0gCY8x9wH0ALVqce980eOBhfuLUplRlEuwXMAX7xdwPrMGGW6Lrfh72A9IAb8/fkZEXTbBvFDac813zrQS+AWZgP3hF6mC/QEVh74XdzK6H/VXeC3zreo9G2A9KUdXvcZUrBxs63q7l1XXdRrjKvAD4oJx1LAosgw33GKApxgRhvwxdsKGIa766VKzFUPSl8So2b9EPTFHrMg9bR0Ut9izXOqVQVO/GFP2YtQCisC0nsF/EENdUdQcuVVwdoI9rqu08saFenB+lf39qqv+96MfCD9vgOHOnf9kqOp7dm1OfoUtDRWomAbuNUKQZcPj0mURkCjAFbIv7/IpToR+b82CwrdqiLo4eQPkHYgT7nv5Ic9frHnTdz8DuoIjDtsyCgQFAL+yX//RAFGwIXsjBFRnYroQ9nNr0Dsf2yRZ9WYXK/ZIWtW6LK+rPc8+x4kq5u4qkyGqgnTGmFXZb507griotlVsIxgZmJDC6AvMbLvy6FcHYH48eZ3kfpdTF7KxJIiIFxpjfA99jm17vi8iWKi+ZUkqpUlWoCSgi/wP+V8VlUUopVQGX4hEDSinl1jS4lVLKzWhwK6WUm9HgVkopN6PBrZRSbkaDWyml3EyVnI/bGFN0HPn5aAAkVWJxqouWu3ppuauXlrvqtRSRCp3vo0qC+0IYY+IqeqKV2kTLXb203NVLy127aFeJUkq5GQ1upZRyM7UxuKfUdAHOk5a7emm5q5eWuxapdX3cSimlylcbW9xKKaXKUWuC2xhzvTFmuzFmlzHm6ZouT1mMMc2NMYuNMfHGmC3GmAmux+sbY34wxux03dY727JqgjHG0xizzhjzret+K2PMKle5Zxtjyr7ycg0yxoQYYz43xmxz1f0V7lDnxpg/uD4nm40xs4wxfrWxzo0x7xtjjhtjNhd7rNT6Nda/Xd/VjcaY8k4QXxPlfsX1OdlojPnKGBNS7LlnXOXebowZVDOlvnC1IrhdV5J/C7gBexG43xhjOtdsqcpUADwmIp2A3sBDrrI+DSwSkXbAItf92mgCEF/s/j+B113lPgGMq5FSnd0kYL6IdMRe+yyeWl7nxpimwCNAjIh0wZ7P/k5qZ51/AFx/2mNl1e8N2OvgtcNervDtaipjaT7gzHL/AHQRkW7ADuAZANf39E7s1U+uBya7ssft1Irgxl42eZeI7BGRfOAT4OYaLlOpROSIiKx1/Z2BDZCm2PJ+6JrtQ2BYzZSwbMaYZsBgYKrrvgH6A5+7Zqmt5S66EOQ0ABHJF5FU3KDOsee89zfGeGGvXHyEWljnIvIz9iKhxZVVvzcDH4m1EggxxjSunpKWVFq5RWSBiBRd228l9nKLYMv9iYjkicheYBc2e9xObQnupsDBYvcTXI/VasaYCOylw1cB4SJyBGy4Y6/QW9u8ATzJqcu0hwKpxT7ktbXeW2Ov1jzd1c0z1RgTSC2vcxE5BLyKvTjpESANe5Vqd6hzKLt+3en7Ohb4zvW3O5W7XLUluCt0JfnaxNjLp38BPCoi6TVdnrMxxgwBjovImuIPlzJrbax3L+yFNt8Wke7Yy8jXqm6R0rj6hG8GWmEvax+I7WY4XW2s8/K4xefGGPMctmtzZtFDpcxW68pdEbUluCt0JfnawhjjjQ3tmSLypevhY0Wbi67b4zVVvjLEAjcZY/Zhu6L6Y1vgIa7NeKi99Z4AJIjIKtf9z7FBXtvr/Fpgr4gkiogD+BK4Eveocyi7fmv999UYMxoYAtwtp8Y81/pyV1RtCe6TV5J37WG/E5hbw2UqlatfeBoQLyKvFXtqLqcu9z4a+Lq6y1YeEXlGRJqJSAS2fn8UkbuBxcBtrtlqXbkBROQocNAY08H10ABgK7W8zrFdJL2NMQGuz01RuWt9nbuUVb9zgVGu0SW9gbSiLpXawBhzPfAUcJOIZBd7ai5wpzHG1xjTCrtz9deaKOMFE5FaMQE3YvcA7waeq+nylFPOq7CbVxuB9a7pRmx/8SJgp+u2fk2XtZx16At86/q7NfbDuwv4DPCt6fKVUeZoIM5V73OAeu5Q58BEYBuwGZgB+NbGOgdmYfvhHdiW6biy6hfb5fCW67u6CTtqpjaVexe2L7vo+/lOsfmfc5V7O3BDTdf7+U565KRSSrmZ2tJVopRSqoI0uJVSys1ocCullJvR4FZKKTejwa2UUm5Gg1sppdyMBrdSSrkZDW6llHIz/w8zr3TO9OofvAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    " #visualize_GOOGL()\n",
    " LSTM_utilizando_GOOGL_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
